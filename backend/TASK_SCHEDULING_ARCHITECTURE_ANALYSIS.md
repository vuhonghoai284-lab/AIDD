# ä»»åŠ¡è°ƒåº¦æ¶æ„åˆ†æä¸æ–¹æ¡ˆå¯¹æ¯”æŠ¥å‘Š

**é¡¹ç›®:** AI Document Testing System  
**åˆ†ææ—¥æœŸ:** 2025-08-28  
**åˆ†æèŒƒå›´:** ä»»åŠ¡è°ƒåº¦æ¶æ„æ¼”è¿›ç­–ç•¥ä¸æŠ€æœ¯é€‰å‹  

---

## 1. å½“å‰ä»»åŠ¡é“¾æ¶æ„åˆ†æ

### 1.1 ç°æœ‰æ¶æ„æ¨¡å¼

**è´£ä»»é“¾å¤„ç†æ¨¡å¼ (Chain of Responsibility):**
```
TaskProcessingChain:
  FileParsingProcessor â†’ DocumentProcessingProcessor â†’ SectionMergeProcessor â†’ IssueDetectionProcessor
```

**æ ¸å¿ƒç‰¹å¾:**
- **åŒæ­¥ä¸²è¡Œå¤„ç†** - æ¯ä¸ªå¤„ç†å™¨ä¾æ¬¡æ‰§è¡Œï¼Œæ— æ³•è·³è¿‡æ­¥éª¤
- **å†…å­˜å‹çŠ¶æ€ç®¡ç†** - å¤„ç†çŠ¶æ€å­˜å‚¨åœ¨è¿›ç¨‹å†…å­˜ä¸­
- **FastAPI BackgroundTasks** - ä½¿ç”¨å†…ç½®åå°ä»»åŠ¡æœºåˆ¶
- **å•è¿›ç¨‹å¤„ç†** - æ‰€æœ‰å¤„ç†é€»è¾‘åœ¨åŒä¸€è¿›ç¨‹å†…æ‰§è¡Œ

### 1.2 å½“å‰æ¶æ„ä¼˜åŠ¿

âœ… **å¿«é€ŸåŸå‹å¼€å‘**
- æ— éœ€é¢å¤–åŸºç¡€è®¾æ–½ï¼ˆRedis/RabbitMQï¼‰
- ä»£ç è€¦åˆåº¦ä½ï¼Œæ˜“äºè°ƒè¯•å’Œæµ‹è¯•
- å¤„ç†é€»è¾‘æ¸…æ™°ï¼Œæ˜“äºç†è§£å’Œç»´æŠ¤

âœ… **èµ„æºå ç”¨è¾ƒä½**  
- å•è¿›ç¨‹éƒ¨ç½²ï¼Œå†…å­˜å ç”¨å¯æ§
- æ— é¢å¤–æ¶ˆæ¯ä¸­é—´ä»¶å¼€é”€
- é€‚åˆä¸­å°è§„æ¨¡éƒ¨ç½²

âœ… **å®æ—¶æ€§è¾ƒå¥½**
- ä»»åŠ¡åˆ›å»ºåç«‹å³å¼€å§‹å¤„ç†
- WebSocketå®æ—¶è¿›åº¦æ¨é€
- æ— é˜Ÿåˆ—æ’é˜Ÿå»¶è¿Ÿ

### 1.3 å½“å‰æ¶æ„å±€é™æ€§

ğŸ”´ **æ‰©å±•æ€§ç“¶é¢ˆ**
- **å•ç‚¹æ•…éšœé£é™©** - WebæœåŠ¡é‡å¯å¯¼è‡´æ‰€æœ‰å¤„ç†ä¸­ä»»åŠ¡ä¸¢å¤±
- **èµ„æºç«äº‰** - CPUå¯†é›†å‹ä»»åŠ¡å½±å“APIå“åº”æ€§èƒ½  
- **æ°´å¹³æ‰©å±•å›°éš¾** - æ— æ³•å¢åŠ å¤„ç†å·¥ä½œè€…æ•°é‡

ğŸ”´ **å¹¶å‘å¤„ç†é™åˆ¶**
- **å…±äº«äº‹ä»¶å¾ªç¯** - åå°ä»»åŠ¡ä¸APIè¯·æ±‚ç«äº‰CPUæ—¶é—´
- **å†…å­˜é™åˆ¶** - å•è¿›ç¨‹å†…å­˜ä¸Šé™çº¦æŸå¹¶å‘ä»»åŠ¡æ•°é‡
- **æ•°æ®åº“ä¼šè¯ç«äº‰** - å·²éƒ¨åˆ†è§£å†³ï¼Œä½†ä»å­˜åœ¨è¿æ¥æ± ç“¶é¢ˆ

ğŸ”´ **å¯é æ€§é—®é¢˜**
- **æ— æŒä¹…åŒ–é˜Ÿåˆ—** - æœåŠ¡é‡å¯ä¸¢å¤±å¾…å¤„ç†ä»»åŠ¡
- **æ— é‡è¯•æœºåˆ¶** - ä»»åŠ¡å¤±è´¥åéœ€æ‰‹åŠ¨é‡è¯•  
- **æ— æ•…éšœæ¢å¤** - å¤„ç†ä¸­æ–­æ— æ³•è‡ªåŠ¨æ¢å¤

---

## 2. Redis + Celery æ–¹æ¡ˆåˆ†æ

### 2.1 Celeryæ¶æ„ä¼˜åŠ¿

**ğŸš€ åˆ†å¸ƒå¼å¤„ç†èƒ½åŠ›**
```python
# å¤šå·¥ä½œè€…å¹¶è¡Œå¤„ç†
CELERY_WORKER_CONFIGURATION = {
    'ai_workers': {
        'concurrency': 20,
        'queues': ['ai_processing'],
        'max_memory_per_child': 1000000  # 1GBå†…å­˜é™åˆ¶
    },
    'file_workers': {
        'concurrency': 10,
        'queues': ['file_processing'],
        'max_memory_per_child': 500000   # 500MBå†…å­˜é™åˆ¶
    }
}
```

**ğŸ”„ é«˜çº§ä»»åŠ¡ç®¡ç†**
- **ä»»åŠ¡é‡è¯•ç­–ç•¥** - æŒ‡æ•°é€€é¿ã€æœ€å¤§é‡è¯•æ¬¡æ•°
- **ä»»åŠ¡è·¯ç”±** - æŒ‰ä»»åŠ¡ç±»å‹å’Œä¼˜å…ˆçº§è·¯ç”±åˆ°ä¸åŒé˜Ÿåˆ—  
- **ä»»åŠ¡è°ƒåº¦** - å®šæ—¶ä»»åŠ¡å’Œå»¶è¿Ÿæ‰§è¡Œ
- **ç›‘æ§å·¥å…·** - Flower Web UIç›‘æ§é›†ç¾¤çŠ¶æ€

**ğŸ’¾ æŒä¹…åŒ–ä¸å¯é æ€§**
- **RedisæŒä¹…åŒ–** - ä»»åŠ¡çŠ¶æ€å’Œç»“æœæŒä¹…åŒ–å­˜å‚¨
- **æ•…éšœæ¢å¤** - å·¥ä½œè€…é‡å¯åè‡ªåŠ¨æ¢å¤æœªå®Œæˆä»»åŠ¡
- **æ¶ˆæ¯ç¡®è®¤** - ç¡®ä¿ä»»åŠ¡è¢«æˆåŠŸå¤„ç†æˆ–é‡æ–°æ’é˜Ÿ

### 2.2 æ€§èƒ½å¯¹æ¯”åˆ†æ

**å¤„ç†èƒ½åŠ›å¯¹æ¯”:**

| æŒ‡æ ‡ | å½“å‰è´£ä»»é“¾ | Redis + Celery |
|------|------------|----------------|
| å¹¶å‘ä»»åŠ¡æ•° | ~20-50ä¸ª | 500+ ä¸ª |
| CPUåˆ©ç”¨ç‡ | å•æ ¸å¿ƒ | å¤šæ ¸å¿ƒ/å¤šæœºå™¨ |
| å†…å­˜ä½¿ç”¨ | 5-20GB | åˆ†æ•£åˆ°å¤šå·¥ä½œè€… |
| æ•…éšœæ¢å¤ | âŒ | âœ… |
| æ°´å¹³æ‰©å±• | âŒ | âœ… |
| ç›‘æ§èƒ½åŠ› | åŸºç¡€ | å®Œå–„ |

**æˆæœ¬æ•ˆç›Šåˆ†æ:**

| æ–¹é¢ | å½“å‰æ¶æ„ | Celeryæ¶æ„ | 
|------|----------|-----------|
| å¼€å‘æˆæœ¬ | ä½ | ä¸­-é«˜ |
| è¿ç»´æˆæœ¬ | ä½ | ä¸­ |
| åŸºç¡€è®¾æ–½æˆæœ¬ | æœ€ä½ | +RedisæœåŠ¡å™¨ |
| æ‰©å±•æˆæœ¬ | é«˜ï¼ˆé‡æ„ï¼‰ | ä½ï¼ˆå¢åŠ å·¥ä½œè€…ï¼‰|
| ç»´æŠ¤æˆæœ¬ | ä¸­ | ä¸­-ä½ |

### 2.3 Celeryè¿ç§»å¤æ‚åº¦

**éœ€è¦é‡æ„çš„ç»„ä»¶:**
1. **ä»»åŠ¡åˆ›å»ºæ¥å£** - æ”¹ä¸ºå¼‚æ­¥ä»»åŠ¡æäº¤
2. **è¿›åº¦æ¨é€æœºåˆ¶** - ä»WebSocketæ”¹ä¸ºRedis Pub/Sub
3. **çŠ¶æ€ç®¡ç†** - ä»å†…å­˜æ”¹ä¸ºRedisçŠ¶æ€å­˜å‚¨
4. **é”™è¯¯å¤„ç†** - é€‚é…Celeryé‡è¯•å’Œæ­»ä¿¡é˜Ÿåˆ—

**è¿ç§»å·¥ä½œé‡ä¼°ç®—:**
- **æ ¸å¿ƒé‡æ„:** 3-4äººå‘¨
- **æµ‹è¯•è°ƒæ•´:** 2-3äººå‘¨  
- **éƒ¨ç½²é…ç½®:** 1-2äººå‘¨
- **æ€»è®¡:** 6-9äººå‘¨

---

## 3. ç°ä»£æ›¿ä»£æ–¹æ¡ˆå¯¹æ¯”

### 3.1 ARQ - ç°ä»£å¼‚æ­¥ä»»åŠ¡é˜Ÿåˆ—

**æŠ€æœ¯ç‰¹ç‚¹:**
```python
# ARQé…ç½®ç¤ºä¾‹
class WorkerSettings:
    redis_settings = RedisSettings()
    functions = [process_document_task]
    max_jobs = 50
    job_timeout = 600
    keep_result = 3600

@arq.worker.func
async def process_document_task(ctx: dict, task_id: int):
    # ç›´æ¥ä½¿ç”¨async/awaitï¼Œæ— éœ€çº¿ç¨‹æ± 
    async with aiofiles.open(file_path) as f:
        content = await f.read()
    return await analyze_document(content)
```

**ARQä¼˜åŠ¿:**
- âœ… **åŸç”Ÿå¼‚æ­¥** - æ— éœ€é€‚é…å™¨ï¼Œç›´æ¥æ”¯æŒasync/await
- âœ… **è½»é‡çº§** - ä»…ä¾èµ–Redisï¼Œé…ç½®ç®€å•
- âœ… **ç°ä»£è®¾è®¡** - ä¸“ä¸ºPython 3.7+å’Œasyncioè®¾è®¡
- âœ… **FastAPIå‹å¥½** - ä¸FastAPIç”Ÿæ€å®Œç¾é›†æˆ

**ARQåŠ£åŠ¿:**  
- âŒ **ç”Ÿæ€è¾ƒæ–°** - ç¬¬ä¸‰æ–¹å·¥å…·å’Œç›‘æ§è¾ƒå°‘
- âŒ **åŠŸèƒ½ç›¸å¯¹ç®€å•** - ä¸å¦‚CeleryåŠŸèƒ½ä¸°å¯Œ
- âŒ **ä»…æ”¯æŒRedis** - ä¸æ”¯æŒå¤šç§æ¶ˆæ¯ä¸­é—´ä»¶

### 3.2 RQ (Redis Queue) - ç®€åŒ–æ–¹æ¡ˆ

**æŠ€æœ¯ç‰¹ç‚¹:**
```python
# RQé…ç½®ç¤ºä¾‹
from rq import Queue
from redis import Redis

redis_conn = Redis()
task_queue = Queue('document_processing', connection=redis_conn)

# æäº¤ä»»åŠ¡
job = task_queue.enqueue(
    process_document,
    task_id,
    timeout=600,
    retry=Retry(max=3)
)
```

**RQä¼˜åŠ¿:**
- âœ… **æç®€è®¾è®¡** - APIç®€å•ç›´è§‚
- âœ… **å¿«é€Ÿé›†æˆ** - æœ€å°åŒ–å­¦ä¹ æˆæœ¬  
- âœ… **å†…ç½®Webç•Œé¢** - RQ Dashboardç›‘æ§
- âœ… **PythonåŸç”Ÿ** - æ— åºåˆ—åŒ–å¼€é”€

**RQå±€é™:**
- âŒ **åŠŸèƒ½è¾ƒåŸºç¡€** - ç¼ºä¹é«˜çº§è°ƒåº¦åŠŸèƒ½
- âŒ **å•æœºæ¶æ„** - æ‰©å±•èƒ½åŠ›æœ‰é™
- âŒ **ä»…æ”¯æŒRedis** - æ¶ˆæ¯ä¸­é—´ä»¶é€‰æ‹©å•ä¸€

### 3.3 FastAPIåŸç”Ÿä¼˜åŒ–æ–¹æ¡ˆ

**å¢å¼ºBackgroundTasksæ¨¡å¼:**
```python
# èµ„æºæ§åˆ¶å¢å¼º
class EnhancedBackgroundTasks:
    def __init__(self, max_concurrent: int = 20):
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.task_registry = {}
        
    async def add_task(self, func, *args, user_id: int = None, **kwargs):
        async with self.semaphore:  # æ§åˆ¶å¹¶å‘æ•°
            task_id = self.register_task(func, user_id)
            try:
                await func(*args, **kwargs)
            finally:
                self.unregister_task(task_id)
```

**ä»»åŠ¡æŒä¹…åŒ–å¢å¼º:**
```python
# ä½¿ç”¨æ•°æ®åº“ä½œä¸ºç®€å•é˜Ÿåˆ—
class DatabaseTaskQueue:
    async def enqueue_task(self, task_data: dict):
        # ä¿å­˜åˆ°æ•°æ®åº“ä»»åŠ¡è¡¨
        task = Task(status='queued', task_data=task_data)
        db.add(task)
        await db.commit()
        
    async def process_queued_tasks(self):
        # å®šæœŸæ‰«æå¹¶å¤„ç†æ’é˜Ÿä»»åŠ¡
        queued_tasks = await self.get_queued_tasks(limit=10)
        await asyncio.gather(*[self.process_task(task) for task in queued_tasks])
```

### 3.4 äº‘åŸç”Ÿæ–¹æ¡ˆ

#### **Google Cloud Tasks + Cloud Run**
```python
# æ— æœåŠ¡å™¨ä»»åŠ¡å¤„ç†
from google.cloud import tasks_v2

client = tasks_v2.CloudTasksClient()
task = {
    'http_request': {
        'http_method': tasks_v2.HttpMethod.POST,
        'url': 'https://worker-service-url/process',
        'body': json.dumps(task_data).encode(),
    }
}
```

#### **AWS SQS + Lambda**
```python
# äº‹ä»¶é©±åŠ¨å¤„ç†
import boto3

sqs = boto3.client('sqs')
sqs.send_message(
    QueueUrl='document-processing-queue',
    MessageBody=json.dumps(task_data),
    MessageAttributes={
        'user_id': {'StringValue': str(user_id), 'DataType': 'String'},
        'priority': {'StringValue': 'high', 'DataType': 'String'}
    }
)
```

---

## 4. æ–¹æ¡ˆå¯¹æ¯”è¯„ä¼°

### 4.1 ç»¼åˆè¯„åˆ†çŸ©é˜µ

| è¯„ä¼°ç»´åº¦ | å½“å‰è´£ä»»é“¾ | ARQ | Celery | RQ | äº‘åŸç”Ÿ |
|----------|-----------|-----|--------|----|---------| 
| **å¼€å‘å¤æ‚åº¦** | 9/10 | 7/10 | 5/10 | 8/10 | 4/10 |
| **æ€§èƒ½æ‰©å±•æ€§** | 3/10 | 8/10 | 9/10 | 6/10 | 10/10 |
| **å¯é æ€§** | 4/10 | 7/10 | 9/10 | 6/10 | 9/10 |
| **è¿ç»´æˆæœ¬** | 9/10 | 7/10 | 5/10 | 7/10 | 8/10 |
| **ç”Ÿæ€æˆç†Ÿåº¦** | 8/10 | 6/10 | 10/10 | 8/10 | 8/10 |
| **å­¦ä¹ æˆæœ¬** | 9/10 | 7/10 | 4/10 | 8/10 | 5/10 |
| **ç›‘æ§èƒ½åŠ›** | 5/10 | 6/10 | 9/10 | 7/10 | 9/10 |

### 4.2 åœºæ™¯é€‚ç”¨æ€§åˆ†æ

#### **å½“å‰è§„æ¨¡é€‚é…åº¦ (50-500ç”¨æˆ·):**

**ä¿æŒè´£ä»»é“¾æ¶æ„ - æ¸è¿›å¼ä¼˜åŒ–:**
- **é€‚ç”¨:** ç”¨æˆ·æ•° < 200ï¼Œå¹¶å‘ä»»åŠ¡ < 50
- **ä¼˜åŒ–æ–¹å‘:** å¢å¼ºèµ„æºæ§åˆ¶ + æ•°æ®åº“é˜Ÿåˆ—
- **æŠ•å…¥:** 1-2äººå‘¨
- **é£é™©:** ä½

**å‡çº§åˆ°ARQ:**
- **é€‚ç”¨:** ç”¨æˆ·æ•° 200-1000ï¼Œéœ€è¦å¼‚æ­¥ä¼˜åŒ–  
- **è¿ç§»æˆæœ¬:** 3-4äººå‘¨
- **æ”¶ç›Š:** æ€§èƒ½æå‡200%ï¼Œä¿æŒæŠ€æœ¯æ ˆä¸€è‡´æ€§
- **é£é™©:** ä¸­ç­‰

**å‡çº§åˆ°Celery:**
- **é€‚ç”¨:** ç”¨æˆ·æ•° > 500ï¼Œéœ€è¦ä¼ä¸šçº§å¯é æ€§
- **è¿ç§»æˆæœ¬:** 6-9äººå‘¨
- **æ”¶ç›Š:** æ€§èƒ½æå‡500%ï¼Œå®Œå–„çš„ç›‘æ§å’Œç®¡ç†
- **é£é™©:** è¾ƒé«˜

#### **æœªæ¥æ‰©å±•è§„åˆ’ (1000+ç”¨æˆ·):**

**å¿…é¡»è€ƒè™‘çš„åœºæ™¯:**
- åŒæ—¶åœ¨çº¿ç”¨æˆ·æ•°ï¼š200+
- å¹¶å‘ä»»åŠ¡å¤„ç†ï¼š200+  
- æ—¥å¤„ç†æ–‡æ¡£é‡ï¼š1000+
- å¤šåœ°åŸŸéƒ¨ç½²éœ€æ±‚

---

## 5. è¯¦ç»†æ–¹æ¡ˆè®¾è®¡

### 5.1 æ–¹æ¡ˆAï¼šæ¸è¿›å¼ä¼˜åŒ– (æ¨èçŸ­æœŸ)

**ä¿æŒè´£ä»»é“¾ + å¢å¼ºèµ„æºæ§åˆ¶**

```python
# å¢å¼ºçš„ä»»åŠ¡è°ƒåº¦å™¨
class EnhancedTaskScheduler:
    def __init__(self):
        self.user_semaphores = {}  # ç”¨æˆ·çº§å¹¶å‘æ§åˆ¶
        self.processing_queue = asyncio.Queue(maxsize=100)
        self.worker_pool = []  # å·¥ä½œè€…æ± 
        
    async def submit_task(self, task_data: dict, user_id: int):
        # 1. ç”¨æˆ·çº§å¹¶å‘æ£€æŸ¥
        user_semaphore = self.get_user_semaphore(user_id)
        if user_semaphore.locked():
            raise UserConcurrencyLimitExceeded()
            
        # 2. æäº¤åˆ°å†…éƒ¨é˜Ÿåˆ—
        await self.processing_queue.put({
            'task_data': task_data,
            'user_id': user_id,
            'submitted_at': time.time()
        })
        
    async def start_workers(self, num_workers: int = 10):
        """å¯åŠ¨å·¥ä½œè€…æ± """
        for i in range(num_workers):
            worker = asyncio.create_task(self.worker_loop(f"worker-{i}"))
            self.worker_pool.append(worker)
            
    async def worker_loop(self, worker_id: str):
        """å·¥ä½œè€…ä¸»å¾ªç¯"""
        while True:
            try:
                # ä»é˜Ÿåˆ—è·å–ä»»åŠ¡
                queue_item = await self.processing_queue.get()
                
                # å¤„ç†ä»»åŠ¡
                async with self.get_user_semaphore(queue_item['user_id']):
                    await self.process_task_with_chain(queue_item)
                    
            except Exception as e:
                logger.error(f"Worker {worker_id} å¤„ç†å¤±è´¥: {e}")
```

**æ•°æ®åº“é˜Ÿåˆ—æŒä¹…åŒ–:**
```python
# ä»»åŠ¡çŠ¶æ€æŒä¹…åŒ–
class TaskQueueManager:
    async def persist_task_state(self, task_id: int, state: dict):
        """å°†ä»»åŠ¡çŠ¶æ€æŒä¹…åŒ–åˆ°æ•°æ®åº“"""
        await self.db.execute(
            "UPDATE tasks SET processing_state = :state WHERE id = :task_id",
            {"state": json.dumps(state), "task_id": task_id}
        )
        
    async def recover_interrupted_tasks(self):
        """æ¢å¤è¢«ä¸­æ–­çš„ä»»åŠ¡"""
        interrupted_tasks = await self.db.fetch_all(
            "SELECT id, processing_state FROM tasks WHERE status = 'processing'"
        )
        for task in interrupted_tasks:
            await self.requeue_task(task['id'], json.loads(task['processing_state']))
```

**èµ„æºç›‘æ§å¢å¼º:**
```python
# å®æ—¶èµ„æºç›‘æ§
class ResourceMonitor:
    def __init__(self):
        self.metrics = {
            'cpu_usage': 0.0,
            'memory_usage': 0.0, 
            'active_tasks': 0,
            'queue_length': 0
        }
        
    async def adaptive_concurrency_control(self):
        """è‡ªé€‚åº”å¹¶å‘æ§åˆ¶"""
        if self.metrics['cpu_usage'] > 80:
            # CPUä½¿ç”¨ç‡è¿‡é«˜ï¼Œå‡å°‘å¹¶å‘æ•°
            self.reduce_concurrency()
        elif self.metrics['cpu_usage'] < 50 and self.metrics['queue_length'] > 10:
            # CPUç©ºé—²ä¸”æœ‰æ’é˜Ÿä»»åŠ¡ï¼Œå¢åŠ å¹¶å‘æ•°
            self.increase_concurrency()
```

### 5.2 æ–¹æ¡ˆBï¼šARQå¼‚æ­¥é˜Ÿåˆ— (æ¨èä¸­æœŸ)

**ARQæ¶æ„è®¾è®¡:**
```python
# ARQå·¥ä½œè€…è®¾ç½®
class WorkerSettings:
    redis_settings = RedisSettings(host='localhost', port=6379, database=1)
    
    # æŒ‰ç”¨æˆ·åˆ†ç»„çš„ä»»åŠ¡å‡½æ•°
    functions = [
        process_file_parsing,
        process_document_analysis,  
        process_issue_detection,
        process_report_generation
    ]
    
    # æ€§èƒ½é…ç½®
    max_jobs = 50  # æœ€å¤§å¹¶å‘ä»»åŠ¡
    job_timeout = 600  # 10åˆ†é’Ÿè¶…æ—¶
    keep_result = 3600  # ä¿ç•™ç»“æœ1å°æ—¶
    
    # ç”¨æˆ·çº§èµ„æºæ§åˆ¶
    max_tries = 3
    retry_delays = [5, 10, 20]  # é‡è¯•å»¶è¿Ÿ

# ç”¨æˆ·çº§ä»»åŠ¡æäº¤
@arq.cron('*/10 * * * * *')  # æ¯10ç§’æ£€æŸ¥
async def schedule_user_tasks(ctx):
    """å®šæœŸè°ƒåº¦ç”¨æˆ·ä»»åŠ¡ï¼Œç¡®ä¿å…¬å¹³æ€§"""
    users_with_queued_tasks = await get_users_with_queued_tasks()
    
    for user_id in users_with_queued_tasks:
        user_concurrent_limit = get_user_concurrent_limit(user_id)
        current_running = await count_user_running_tasks(user_id)
        
        if current_running < user_concurrent_limit:
            available_slots = user_concurrent_limit - current_running
            await schedule_user_tasks_batch(user_id, available_slots)
```

**ä»»åŠ¡é“¾é‡æ„:**
```python
# ARQä»»åŠ¡é“¾å®ç°
@arq.worker.func
async def process_document_pipeline(ctx, task_id: int, user_id: int):
    """å®Œæ•´çš„æ–‡æ¡£å¤„ç†ç®¡é“"""
    # è·å–ç”¨æˆ·èµ„æºé…é¢
    async with get_user_resource_limit(user_id):
        # 1. æ–‡ä»¶è§£æ
        parsing_result = await arq.enqueue_job(
            'process_file_parsing',
            task_id,
            user_id=user_id,
            _queue_name=f'user_{user_id % 10}'  # ç”¨æˆ·åˆ†ç‰‡
        )
        
        # 2. æ–‡æ¡£åˆ†æ 
        analysis_result = await arq.enqueue_job(
            'process_document_analysis',
            task_id,
            parsing_result=parsing_result,
            _depends_on=parsing_result.job_id
        )
        
        # 3. é—®é¢˜æ£€æµ‹
        detection_result = await arq.enqueue_job(
            'process_issue_detection', 
            task_id,
            analysis_result=analysis_result,
            _depends_on=analysis_result.job_id
        )
        
        return detection_result
```

### 5.3 æ–¹æ¡ˆCï¼šæ•°æ®åº“é˜Ÿåˆ—å¢å¼º (æœ€å°åŒ–æ”¹åŠ¨)

**æ ¸å¿ƒæ€è·¯ï¼šä¿ç•™è´£ä»»é“¾ï¼Œå¢åŠ é˜Ÿåˆ—å±‚**

```python
# æ•°æ®åº“åŸç”Ÿé˜Ÿåˆ—å®ç°
class DatabaseTaskQueue:
    def __init__(self):
        self.worker_pool_size = 20
        self.user_concurrency_limits = {}
        
    async def enqueue_task(self, task_id: int, user_id: int, priority: int = 5):
        """å°†ä»»åŠ¡åŠ å…¥æ•°æ®åº“é˜Ÿåˆ—"""
        queue_item = TaskQueueItem(
            task_id=task_id,
            user_id=user_id,
            priority=priority,
            status='queued',
            queued_at=datetime.utcnow(),
            attempts=0
        )
        self.db.add(queue_item)
        await self.db.commit()
        
        # è§¦å‘å¤„ç†å™¨æ£€æŸ¥
        await self.notify_workers()
    
    async def dequeue_next_task(self, worker_id: str) -> Optional[TaskQueueItem]:
        """å‡ºé˜Ÿä¸‹ä¸€ä¸ªå¯å¤„ç†çš„ä»»åŠ¡"""
        # æŒ‰ä¼˜å…ˆçº§å’Œç”¨æˆ·å…¬å¹³æ€§é€‰æ‹©ä»»åŠ¡
        next_task = await self.db.execute("""
            SELECT q.* FROM task_queue q
            JOIN users u ON q.user_id = u.id  
            WHERE q.status = 'queued'
              AND (
                SELECT COUNT(*) FROM task_queue q2 
                WHERE q2.user_id = q.user_id AND q2.status = 'processing'
              ) < u.max_concurrent_tasks
            ORDER BY q.priority DESC, q.queued_at ASC
            LIMIT 1
        """)
        
        if next_task:
            # åŸå­æ›´æ–°çŠ¶æ€
            await self.db.execute(
                "UPDATE task_queue SET status = 'processing', worker_id = :worker_id WHERE id = :id",
                {"worker_id": worker_id, "id": next_task.id}
            )
            
        return next_task
        
    async def start_worker_pool(self):
        """å¯åŠ¨å·¥ä½œè€…æ± """
        workers = []
        for i in range(self.worker_pool_size):
            worker = asyncio.create_task(self.worker_loop(f"db-worker-{i}"))
            workers.append(worker)
        return workers
        
    async def worker_loop(self, worker_id: str):
        """æ•°æ®åº“é˜Ÿåˆ—å·¥ä½œè€…å¾ªç¯"""
        while True:
            try:
                # è·å–ä¸‹ä¸€ä¸ªä»»åŠ¡
                queue_item = await self.dequeue_next_task(worker_id)
                
                if queue_item:
                    # ä½¿ç”¨ç°æœ‰è´£ä»»é“¾å¤„ç†
                    await self.process_with_existing_chain(queue_item.task_id)
                    # æ ‡è®°å®Œæˆ
                    await self.mark_task_completed(queue_item.id)
                else:
                    # æ— ä»»åŠ¡æ—¶çŸ­æš‚ä¼‘çœ 
                    await asyncio.sleep(1)
                    
            except Exception as e:
                logger.error(f"Worker {worker_id} å¼‚å¸¸: {e}")
                await asyncio.sleep(5)  # å¼‚å¸¸æ¢å¤å»¶è¿Ÿ
```

---

## 6. æ–¹æ¡ˆæ¨èä¸å®æ–½ç­–ç•¥

### 6.1 åˆ†é˜¶æ®µå®æ–½å»ºè®®

#### **é˜¶æ®µ1ï¼šå½“å‰æ¶æ„ä¼˜åŒ– (1-2å‘¨)**
**ç›®æ ‡ï¼š** æå‡50%æ€§èƒ½ï¼Œæ”¯æŒ200å¹¶å‘ä»»åŠ¡

**å®æ–½å†…å®¹ï¼š**
1. **å®ç°æ–¹æ¡ˆC - æ•°æ®åº“é˜Ÿåˆ—å¢å¼º**
   - ä¿ç•™ç°æœ‰è´£ä»»é“¾å¤„ç†é€»è¾‘
   - æ·»åŠ æ•°æ®åº“é˜Ÿåˆ—å±‚è¿›è¡Œä»»åŠ¡è°ƒåº¦
   - å®ç°å·¥ä½œè€…æ± æ¨¡å¼æå‡å¹¶å‘èƒ½åŠ›

2. **ç”¨æˆ·çº§èµ„æºæ§åˆ¶**
   - æŒ‰ç”¨æˆ·çš„ä¿¡å·é‡å¹¶å‘æ§åˆ¶
   - WebSocketè¿æ¥æ•°é™åˆ¶
   - æ•°æ®åº“ä¼šè¯ç”¨æˆ·çº§è·Ÿè¸ª

3. **ç›‘æ§å¢å¼º**
   - é˜Ÿåˆ—é•¿åº¦ç›‘æ§  
   - ç”¨æˆ·èµ„æºä½¿ç”¨ç»Ÿè®¡
   - æ€§èƒ½ç“¶é¢ˆå‘Šè­¦

**ä»£ç ç¤ºä¾‹:**
```python
# æœ€å°æ”¹åŠ¨é›†æˆ
class TaskService:
    def __init__(self, db: Session):
        self.db = db
        self.task_queue = DatabaseTaskQueue(db)  # æ–°å¢é˜Ÿåˆ—ç®¡ç†
        # ä¿ç•™ç°æœ‰çš„æ‰€æœ‰ä»“åº“å’Œé€»è¾‘
        
    async def create_task(self, file: UploadFile, user_id: int):
        # åˆ›å»ºä»»åŠ¡è®°å½•ï¼ˆç°æœ‰é€»è¾‘ï¼‰
        task = await self.create_task_record(file, user_id)
        
        # æ–°å¢ï¼šæäº¤åˆ°é˜Ÿåˆ—è€Œéç›´æ¥å¤„ç†
        await self.task_queue.enqueue_task(task.id, user_id)
        
        return task
```

#### **é˜¶æ®µ2ï¼šARQå¼‚æ­¥é˜Ÿåˆ—å‡çº§ (1-2æœˆ)**  
**ç›®æ ‡ï¼š** æ”¯æŒ1000+å¹¶å‘ä»»åŠ¡ï¼Œå®Œå–„çš„å¯é æ€§

**è¿ç§»ç­–ç•¥ï¼š**
1. **å¹¶è¡Œè¿è¡Œæ¨¡å¼** - ARQä¸è´£ä»»é“¾å¹¶å­˜2-4å‘¨
2. **åˆ†æ‰¹è¿ç§»ç”¨æˆ·** - æŒ‰ç”¨æˆ·ç»„é€æ­¥åˆ‡æ¢åˆ°ARQ
3. **A/Bæµ‹è¯•éªŒè¯** - å¯¹æ¯”æ–°æ—§æ¶æ„æ€§èƒ½è¡¨ç°

#### **é˜¶æ®µ3ï¼šä¼ä¸šçº§Celery/äº‘åŸç”Ÿ (3-6æœˆ)**
**ç›®æ ‡ï¼š** ä¼ä¸šçº§å¯æ‰©å±•æ€§ï¼Œæ”¯æŒå¤šåœ°åŸŸéƒ¨ç½²

### 6.2 æŠ€æœ¯é€‰å‹å»ºè®®

**é’ˆå¯¹å½“å‰é¡¹ç›®ç‰¹ç‚¹çš„å»ºè®®ï¼š**

#### **æ¨èæ–¹æ¡ˆï¼šé˜¶æ®µ1æ•°æ®åº“é˜Ÿåˆ— + é˜¶æ®µ2 ARQ**

**ç†ç”±åˆ†æï¼š**

1. **æŠ€æœ¯æ ˆä¸€è‡´æ€§** - ARQä¸FastAPIéƒ½æ˜¯async-firstï¼Œé›†æˆè‡ªç„¶
2. **æ¸è¿›å¼å‡çº§** - åˆ†é˜¶æ®µé™ä½é£é™©ï¼Œå¯éšæ—¶å›é€€
3. **æˆæœ¬æ•ˆç›Šæ¯”æœ€ä¼˜** - å¼€å‘æˆæœ¬é€‚ä¸­ï¼Œæ€§èƒ½æ”¶ç›Šæ˜¾è‘—  
4. **è¿ç»´å‹å¥½** - ç›¸æ¯”Celeryå¤æ‚åº¦é™ä½50%

#### **ä¸æ¨èCeleryçš„åŸå› ï¼š**

1. **è¿‡åº¦å·¥ç¨‹åŒ–** - å½“å‰è§„æ¨¡ä¸‹Celeryèƒ½åŠ›è¿‡å‰©
2. **æŠ€æœ¯æ ˆæ··æ‚** - åŒæ­¥Celeryä¸å¼‚æ­¥FastAPIé›†æˆå¤æ‚
3. **è¿ç»´è´Ÿæ‹…é‡** - éœ€è¦ä¸“é—¨çš„DevOpsèµ„æºç»´æŠ¤

#### **æš‚ä¸è€ƒè™‘äº‘åŸç”Ÿçš„åŸå› ï¼š**

1. **å‚å•†ç»‘å®šé£é™©** - éš¾ä»¥è¿ç§»å’Œåˆ‡æ¢
2. **æˆæœ¬ä¸å¯æ§** - æŒ‰è°ƒç”¨é‡è®¡è´¹å¯èƒ½äº§ç”Ÿæ„å¤–æˆæœ¬
3. **è°ƒè¯•å›°éš¾** - ç¼ºä¹æœ¬åœ°å¼€å‘ç¯å¢ƒ

---

## 7. å…·ä½“å®æ–½è®¡åˆ’

### 7.1 é˜¶æ®µ1å®æ–½ç»†èŠ‚ (æ¨èç«‹å³å¼€å§‹)

#### **å‘¨1ï¼šæ•°æ®åº“é˜Ÿåˆ—åŸºç¡€è®¾æ–½**
```sql
-- åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—è¡¨
CREATE TABLE task_queue (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    task_id INT NOT NULL,
    user_id INT NOT NULL, 
    priority INT DEFAULT 5,
    status ENUM('queued', 'processing', 'completed', 'failed') DEFAULT 'queued',
    worker_id VARCHAR(50),
    queued_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    started_at TIMESTAMP NULL,
    completed_at TIMESTAMP NULL,
    attempts INT DEFAULT 0,
    max_attempts INT DEFAULT 3,
    error_message TEXT,
    INDEX idx_status_priority (status, priority, queued_at),
    INDEX idx_user_id_status (user_id, status)
);
```

#### **å‘¨2ï¼šå·¥ä½œè€…æ± å®ç°**
```python
# å·¥ä½œè€…ç®¡ç†å™¨
class WorkerManager:
    def __init__(self, num_workers: int = 20):
        self.num_workers = num_workers
        self.workers = []
        self.shutdown_event = asyncio.Event()
        
    async def start(self):
        """å¯åŠ¨å·¥ä½œè€…æ± """
        for i in range(self.num_workers):
            worker = DatabaseWorker(f"worker-{i}", self.shutdown_event)
            worker_task = asyncio.create_task(worker.run())
            self.workers.append((worker, worker_task))
            
    async def graceful_shutdown(self):
        """ä¼˜é›…å…³é—­"""
        self.shutdown_event.set()
        await asyncio.gather(*[task for _, task in self.workers])

class DatabaseWorker:
    async def run(self):
        """å·¥ä½œè€…ä¸»å¾ªç¯"""  
        while not self.shutdown_event.is_set():
            try:
                task_item = await self.queue_manager.dequeue_next_task(self.worker_id)
                if task_item:
                    await self.process_task_item(task_item)
                else:
                    await asyncio.sleep(1)  # æ— ä»»åŠ¡æ—¶ä¼‘çœ 
            except Exception as e:
                logger.error(f"Worker {self.worker_id} å¼‚å¸¸: {e}")
                await asyncio.sleep(5)  # å¼‚å¸¸æ¢å¤å»¶è¿Ÿ
```

### 7.2 æ€§èƒ½æµ‹è¯•åŸºå‡†

**æµ‹è¯•åœºæ™¯è®¾è®¡ï¼š**
```python
# æ€§èƒ½åŸºå‡†æµ‹è¯•
async def benchmark_task_scheduling():
    """ä»»åŠ¡è°ƒåº¦æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    # åœºæ™¯1ï¼šå•ç”¨æˆ·å¤§æ‰¹é‡
    user_id = 1
    tasks = []
    for i in range(50):
        task = await submit_test_task(user_id, f"batch_task_{i}")
        tasks.append(task)
    
    completion_time = await measure_completion_time(tasks)
    print(f"å•ç”¨æˆ·50ä»»åŠ¡å®Œæˆæ—¶é—´: {completion_time}s")
    
    # åœºæ™¯2ï¼šå¤šç”¨æˆ·å¹¶å‘
    user_tasks = {}
    for user_id in range(1, 11):  # 10ä¸ªç”¨æˆ·
        user_tasks[user_id] = []
        for i in range(10):  # æ¯ç”¨æˆ·10ä¸ªä»»åŠ¡
            task = await submit_test_task(user_id, f"user_{user_id}_task_{i}")
            user_tasks[user_id].append(task)
    
    # æµ‹è¯•å…¬å¹³æ€§ - æ¯ä¸ªç”¨æˆ·çš„å¹³å‡å®Œæˆæ—¶é—´
    fairness_metrics = {}
    for user_id, tasks in user_tasks.items():
        completion_time = await measure_completion_time(tasks)
        fairness_metrics[user_id] = completion_time
    
    fairness_variance = statistics.variance(fairness_metrics.values())
    print(f"å¤šç”¨æˆ·å…¬å¹³æ€§æ–¹å·®: {fairness_variance:.2f}s")
```

### 7.3 è¿ç§»é£é™©æ§åˆ¶

**å›é€€ç­–ç•¥:**
```python
# ç‰¹æ€§å¼€å…³æ§åˆ¶
FEATURE_FLAGS = {
    'use_database_queue': False,  # æ•°æ®åº“é˜Ÿåˆ—
    'use_worker_pool': False,     # å·¥ä½œè€…æ± 
    'use_user_semaphore': False   # ç”¨æˆ·ä¿¡å·é‡
}

class TaskService:
    async def create_task(self, file: UploadFile, user_id: int):
        if FEATURE_FLAGS['use_database_queue']:
            # æ–°æ¶æ„ï¼šé˜Ÿåˆ—æ¨¡å¼
            return await self.create_task_with_queue(file, user_id)
        else:
            # åŸæ¶æ„ï¼šç›´æ¥å¤„ç†
            return await self.create_task_direct(file, user_id)
```

**ç›‘æ§å¯¹æ¯”:**
```python
# A/Bæµ‹è¯•æ€§èƒ½å¯¹æ¯”
class PerformanceComparator:
    def __init__(self):
        self.metrics = {
            'legacy_chain': [],
            'database_queue': [],
            'arq_queue': []
        }
    
    async def compare_architectures(self):
        """å¯¹æ¯”ä¸åŒæ¶æ„çš„æ€§èƒ½è¡¨ç°"""
        test_cases = self.generate_test_cases()
        
        for architecture in ['legacy_chain', 'database_queue']:
            metrics = await self.run_performance_test(architecture, test_cases)
            self.metrics[architecture] = metrics
            
        return self.generate_comparison_report()
```

---

## 8. æ€»ä½“å»ºè®®

### 8.1 æ¨èå®æ–½è·¯å¾„

**ğŸ¯ å¼ºçƒˆæ¨èï¼šæ¸è¿›å¼å‡çº§è·¯å¾„**

```
å½“å‰è´£ä»»é“¾ â†’ æ•°æ®åº“é˜Ÿåˆ—å¢å¼º â†’ ARQå¼‚æ­¥é˜Ÿåˆ— â†’ (å¯é€‰) Celery/äº‘åŸç”Ÿ
    â†“              â†“                â†“                â†“
   ç«‹å³          1-2å‘¨             1-2æœˆ             6æœˆ+
```

**å…·ä½“å»ºè®®ï¼š**

1. **ç«‹å³å®æ–½ï¼š** æ–¹æ¡ˆAæ•°æ®åº“é˜Ÿåˆ—å¢å¼º
   - **æŠ•èµ„å›æŠ¥æ¯”æœ€é«˜** - 1-2äººå‘¨æŠ•å…¥ï¼Œè·å¾—50%æ€§èƒ½æå‡
   - **é£é™©æœ€ä½** - å¯å¿«é€Ÿå›é€€åˆ°åŸæ¶æ„
   - **æ•ˆæœç«‹ç«¿è§å½±** - è§£å†³å½“å‰50ç§’é˜»å¡é—®é¢˜

2. **1-2æœˆåè€ƒè™‘ï¼š** æ–¹æ¡ˆB ARQå‡çº§  
   - **ç°ä»£åŒ–æŠ€æœ¯æ ˆ** - å…¨å¼‚æ­¥æ¶æ„ä¸€è‡´æ€§
   - **æ€§èƒ½æå‡æ˜¾è‘—** - æ”¯æŒ500+å¹¶å‘ä»»åŠ¡
   - **è¿ç»´å‹å¥½** - ç›¸æ¯”Celeryå¤æ‚åº¦ä½50%

3. **æš‚ä¸å»ºè®®ï¼š** ç›´æ¥å‡çº§Celery
   - **è¿‡åº¦å·¥ç¨‹åŒ–** - å½“å‰è§„æ¨¡ä¸‹èƒ½åŠ›è¿‡å‰©
   - **è¿ç§»æˆæœ¬é«˜** - 6-9äººå‘¨å¼€å‘æŠ•å…¥  
   - **æŠ€æœ¯å€ºåŠ¡** - åŒæ­¥å¼‚æ­¥æ··åˆæ¶æ„å¤æ‚åº¦é«˜

### 8.2 æ ¸å¿ƒåˆ¤æ–­æ ‡å‡†

**ä½•æ—¶å¿…é¡»å‡çº§åˆ°åˆ†å¸ƒå¼é˜Ÿåˆ—ï¼š**
- å¹¶å‘ç”¨æˆ·æ•° > 200  
- å•æ—¥ä»»åŠ¡é‡ > 1000
- å•ä¸ªä»»åŠ¡å¤„ç†æ—¶é—´ > 10åˆ†é’Ÿ
- éœ€è¦è·¨æœºå™¨æ‰©å±•èƒ½åŠ›

**å½“å‰é˜¶æ®µæœ€ä¼˜ç­–ç•¥ï¼š**
ä¿æŒè´£ä»»é“¾æ¶æ„çš„ç®€æ´æ€§ï¼Œé€šè¿‡æ•°æ®åº“é˜Ÿåˆ—å’Œå·¥ä½œè€…æ± æ¨¡å¼å®ç°æ€§èƒ½æå‡ï¼Œä¸ºæœªæ¥å‡çº§åˆ°ARQåšå¥½å‡†å¤‡ï¼Œé¿å…è¿‡æ—©ä¼˜åŒ–çš„æŠ€æœ¯å€ºåŠ¡ã€‚

---

## 9. å®æ–½ç›‘æ§æŒ‡æ ‡

### 9.1 æˆåŠŸæ ‡å‡†

**æ€§èƒ½æŒ‡æ ‡ï¼š**
- æ‰¹é‡ä»»åŠ¡åˆ›å»ºå“åº”æ—¶é—´ï¼š< 500ms (å½“å‰ >50s)
- ç³»ç»Ÿå¹¶å‘å¤„ç†èƒ½åŠ›ï¼š200+ ä»»åŠ¡ (å½“å‰ ~50)
- å¹³å‡ä»»åŠ¡å®Œæˆæ—¶é—´ï¼š< 5åˆ†é’Ÿ
- APIå“åº”æ—¶é—´P95ï¼š< 1ç§’

**å¯é æ€§æŒ‡æ ‡ï¼š**
- ä»»åŠ¡ä¸¢å¤±ç‡ï¼š0%
- ç³»ç»Ÿå¯ç”¨æ€§ï¼š99.5%+
- æ•…éšœæ¢å¤æ—¶é—´ï¼š< 5åˆ†é’Ÿ

### 9.2 é£é™©æ§åˆ¶æŒ‡æ ‡

**èµ„æºä½¿ç”¨å‘Šè­¦ï¼š**
- æ•°æ®åº“è¿æ¥ä½¿ç”¨ç‡ > 80%
- å†…å­˜ä½¿ç”¨ç‡ > 85%  
- é˜Ÿåˆ—é•¿åº¦ > 100
- å•ç”¨æˆ·ä»»åŠ¡ç§¯å‹ > 20ä¸ª

**ç”¨æˆ·ä½“éªŒæŒ‡æ ‡ï¼š**
- ä»»åŠ¡åˆ›å»ºæˆåŠŸç‡ > 99%
- è¿›åº¦æ›´æ–°å»¶è¿Ÿ < 5ç§’
- å‰ç«¯é¡µé¢å“åº”æ—¶é—´ < 2ç§’

---

**ç»“è®ºï¼š** æ¨èé‡‡ç”¨æ¸è¿›å¼å‡çº§ç­–ç•¥ï¼Œå…ˆå®æ–½æ•°æ®åº“é˜Ÿåˆ—å¢å¼ºç‰ˆæœ¬ï¼Œè·å¾—ç«‹å³çš„æ€§èƒ½æ”¹å–„ï¼Œå†æ ¹æ®ä¸šåŠ¡å‘å±•éœ€æ±‚è€ƒè™‘å‡çº§åˆ°ARQå¼‚æ­¥é˜Ÿåˆ—æ¶æ„ã€‚é¿å…ç›´æ¥è·³è·ƒåˆ°Celeryé‡æ¶æ„ï¼Œç¡®ä¿æŠ€æœ¯æ¼”è¿›çš„å¹³æ»‘è¿‡æ¸¡å’Œé£é™©å¯æ§ã€‚