"""
æ–°ä»»åŠ¡å¤„ç†å™¨ - ä½¿ç”¨è´£ä»»é“¾æ¨¡å¼
"""
import asyncio
import json
import time
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Any
from sqlalchemy.orm import Session

from app.repositories.task import TaskRepository
from app.repositories.issue import IssueRepository
from app.repositories.ai_output import AIOutputRepository
from app.repositories.file_info import FileInfoRepository
from app.repositories.ai_model import AIModelRepository
from app.core.config import get_settings
from app.services.websocket import manager
from app.models import TaskLog
from app.services.processing_chain import TaskProcessingChain
from app.services.ai_service_providers.service_provider_factory import ai_service_provider_factory


class NewTaskProcessor:
    """æ–°ä»»åŠ¡å¤„ç†å™¨ - ä½¿ç”¨è´£ä»»é“¾æ¨¡å¼"""
    
    def __init__(self, db: Session):
        self.db = db
        self.task_repo = TaskRepository(db)
        self.issue_repo = IssueRepository(db)
        self.ai_output_repo = AIOutputRepository(db)
        self.file_repo = FileInfoRepository(db)
        self.model_repo = AIModelRepository(db)
        self.settings = get_settings()
        self.start_time = None  # è®°å½•ä»»åŠ¡å¼€å§‹æ—¶é—´
    
    async def process_task(self, task_id: int):
        """å¤„ç†ä»»åŠ¡"""
        try:
            # è®°å½•ä»»åŠ¡å¼€å§‹æ—¶é—´ï¼ˆä½¿ç”¨UTCæ—¶é—´æˆ³ï¼‰
            self.start_time = time.time()
            
            # è®°å½•å¼€å§‹æ—¥å¿—
            await self._log(task_id, "INFO", "å¼€å§‹å¤„ç†ä»»åŠ¡", "åˆå§‹åŒ–", 0)
            
            # è·å–ä»»åŠ¡ä¿¡æ¯
            task = self.task_repo.get(task_id)
            if not task:
                raise ValueError(f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}")
            
            # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
            self.task_repo.update(task_id, status="processing", progress=10)
            await manager.send_status(task_id, "processing")
            
            # å‡†å¤‡å¤„ç†ä¸Šä¸‹æ–‡
            context = await self._prepare_context(task_id, task)
            
            # è·å–ä»»åŠ¡å…³è”çš„AIæ¨¡å‹å¹¶æ‰¾åˆ°å¯¹åº”çš„ç´¢å¼•
            task_model_index = self.settings.default_model_index  # é»˜è®¤å€¼
            if task.model_id:
                # æ ¹æ®model_idæŸ¥æ‰¾æ¨¡å‹é…ç½®
                ai_model = self.model_repo.get_by_id(task.model_id)
                if ai_model:
                    # åœ¨settingsçš„æ¨¡å‹åˆ—è¡¨ä¸­æŸ¥æ‰¾å¯¹åº”çš„ç´¢å¼•
                    for index, model_config in enumerate(self.settings.ai_models):
                        config_model_name = model_config.get('config', {}).get('model')
                        config_provider = model_config.get('provider')
                        
                        # ä½¿ç”¨model_nameå’Œproviderè¿›è¡ŒåŒ¹é…
                        if (config_model_name == ai_model.model_name and 
                            config_provider == ai_model.provider):
                            task_model_index = index
                            self.logger.info(f"ğŸ¯ æ‰¾åˆ°ç”¨æˆ·é€‰æ‹©çš„æ¨¡å‹: {ai_model.label} (model: {ai_model.model_name}, provider: {ai_model.provider}, ç´¢å¼•: {index})")
                            break
                    else:
                        self.logger.warning(f"âš ï¸ æœªæ‰¾åˆ°åŒ¹é…çš„æ¨¡å‹é…ç½®ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹ã€‚æ¨¡å‹ä¿¡æ¯: label={ai_model.label}, model={ai_model.model_name}, provider={ai_model.provider}")
                        self.logger.info("å¯ç”¨æ¨¡å‹é…ç½®:")
                        for i, cfg in enumerate(self.settings.ai_models):
                            cfg_model = cfg.get('config', {}).get('model', 'unknown')
                            cfg_provider = cfg.get('provider', 'unknown')
                            self.logger.info(f"  [{i}] {cfg.get('label', 'unknown')} - {cfg_provider} ({cfg_model})")
                else:
                    self.logger.warning(f"âš ï¸ ä»»åŠ¡å…³è”çš„æ¨¡å‹ä¸å­˜åœ¨(model_id={task.model_id})ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹")
            
            # åˆ›å»ºAIæœåŠ¡æä¾›è€…
            ai_service_provider = ai_service_provider_factory.create_provider(
                settings=self.settings,
                model_index=task_model_index,
                db_session=self.db
            )
            
            # åˆ›å»ºå¤„ç†é“¾
            processing_chain = TaskProcessingChain(ai_service_provider)
            
            # æ‰§è¡Œå¤„ç†é“¾
            await self._log(task_id, "INFO", f"ä½¿ç”¨AIæœåŠ¡: {ai_service_provider.get_provider_name()}", "åˆå§‹åŒ–", 10)
            
            async def progress_callback(message: str, progress: int):
                """è¿›åº¦å›è°ƒå‡½æ•°"""
                # è®°å½•æ—¥å¿—å¹¶æ¨é€æ¶ˆæ¯
                await self._log(task_id, "INFO", message, "å¤„ç†ä¸­", progress)
                # æ›´æ–°ä»»åŠ¡è¿›åº¦
                self.task_repo.update(task_id, progress=progress)
                # å‘é€è¿›åº¦çŠ¶æ€æ›´æ–°ï¼ˆä¸é‡å¤å‘é€æ¶ˆæ¯ï¼‰
                await manager.send_status(task_id, "processing")
            
            context['progress_callback'] = progress_callback
            
            # æ‰§è¡Œå®Œæ•´çš„å¤„ç†é“¾
            result = await processing_chain.execute(context, progress_callback)
            
            if not result.success:
                raise ValueError(f"ä»»åŠ¡å¤„ç†å¤±è´¥: {result.error}")
            
            # ä¿å­˜å¤„ç†ç»“æœ
            await self._save_processing_results(task_id, context, result)
            
            # å®Œæˆä»»åŠ¡
            # ä½¿ç”¨ä»»åŠ¡å®é™…å¼€å§‹æ—¶é—´è®¡ç®—è€—æ—¶ï¼Œé¿å…æ—¶åŒºè½¬æ¢é—®é¢˜
            processing_time = time.time() - self.start_time if self.start_time else 0
            self.task_repo.update(
                task_id, 
                status="completed",
                progress=100,
                processing_time=processing_time,
                completed_at=datetime.utcnow()  # ä½¿ç”¨UTCæ—¶é—´ä¿æŒä¸€è‡´æ€§
            )
            await manager.send_progress(task_id, 100, "å®Œæˆ")
            await manager.send_status(task_id, "completed")
            await self._log(task_id, "INFO", f"ä»»åŠ¡å¤„ç†å®Œæˆï¼Œè€—æ—¶{processing_time:.2f}ç§’", "å®Œæˆ", 100)
            
        except Exception as e:
            # è®°å½•é”™è¯¯
            await self._log(task_id, "ERROR", f"ä»»åŠ¡å¤„ç†å¤±è´¥: {str(e)}", "é”™è¯¯", 0)
            await manager.send_status(task_id, "failed")
            self.task_repo.update(
                task_id, 
                status="failed", 
                error_message=str(e)
            )
            raise
    
    async def _prepare_context(self, task_id: int, task) -> Dict[str, Any]:
        """å‡†å¤‡å¤„ç†ä¸Šä¸‹æ–‡"""
        context = {
            'task_id': task_id
        }
        
        # è·å–æ–‡ä»¶ä¿¡æ¯
        file_info = None
        if task.file_id:
            file_info = self.file_repo.get_by_id(task.file_id)
        
        if file_info:
            context['file_path'] = file_info.file_path
            context['file_name'] = file_info.original_name
            await self._log(task_id, "INFO", f"æ­£åœ¨å¤„ç†æ–‡æ¡£: {file_info.original_name}", "æ–‡æ¡£è§£æ", 10)
        else:
            context['file_name'] = "æµ‹è¯•æ–‡æ¡£"
            await self._log(task_id, "INFO", "ä½¿ç”¨æµ‹è¯•æ¨¡å¼æ–‡æ¡£", "æ–‡æ¡£è§£æ", 10)
        
        return context
    
    async def _save_processing_results(self, task_id: int, context: Dict[str, Any], result):
        """ä¿å­˜å¤„ç†ç»“æœ"""
        await self._log(task_id, "INFO", "æ­£åœ¨ä¿å­˜å¤„ç†ç»“æœ", "ä¿å­˜ç»“æœ", 85)
        
        # ä¿å­˜æ–‡ä»¶è§£æç»“æœï¼ˆéAIæ­¥éª¤ï¼Œä¿å­˜å¤„ç†è®°å½•ï¼‰
        if 'file_parsing_result' in context:
            self._save_ai_output(
                task_id=task_id,
                operation_type="file_parsing",
                input_text=str(context.get('file_path', 'test')),
                result={
                    'status': 'success',
                    'data': {'content': context['file_parsing_result'][:1000]},  # åªä¿å­˜å‰1000å­—ç¬¦
                    'processing_stage': 'file_parsing'
                }
            )
        
        # ç« èŠ‚åˆå¹¶ç»“æœè®°å½•ï¼ˆéAIæ­¥éª¤ï¼Œä¿å­˜å¤„ç†è®°å½•ï¼‰
        if 'section_merge_result' in context:
            original_count = len(context.get('document_processing_result', []))
            merged_count = len(context['section_merge_result'])
            self._save_ai_output(
                task_id=task_id,
                operation_type="section_merge",
                input_text=f"åŸå§‹ç« èŠ‚æ•°: {original_count}",
                result={
                    'status': 'success',
                    'data': {
                        'original_sections_count': original_count,
                        'merged_sections_count': merged_count,
                        'merge_ratio': merged_count / original_count if original_count > 0 else 0,
                        'merged_sections': context['section_merge_result'][:3]  # ä¿å­˜å‰3ä¸ªåˆå¹¶ç« èŠ‚çš„æ¦‚è¦
                    },
                    'processing_stage': 'section_merge'
                }
            )
        
        # ä¿å­˜é—®é¢˜åˆ°æ•°æ®åº“ï¼ˆé—®é¢˜æ£€æµ‹çš„AIè¾“å‡ºå·²ç”±IssueDetectorä¿å­˜ï¼‰
        if 'issue_detection_result' in context:
            issues = context['issue_detection_result']
            
            # ä¿å­˜é—®é¢˜åˆ°æ•°æ®åº“
            issue_count = len(issues) if issues else 0
            await self._log(task_id, "INFO", f"æ£€æµ‹åˆ°{issue_count}ä¸ªé—®é¢˜", "ä¿å­˜ç»“æœ", 90)
            
            for issue in (issues or []):
                self.issue_repo.create(
                    task_id=task_id,
                    issue_type=issue.get('issue_type', 'æœªçŸ¥'),
                    description=issue.get('description', ''),
                    location=issue.get('location', ''),
                    severity=issue.get('severity', 'ä¸€èˆ¬'),
                    confidence=issue.get('confidence'),
                    suggestion=issue.get('suggestion', ''),
                    original_text=issue.get('original_text'),
                    user_impact=issue.get('user_impact'),
                    reasoning=issue.get('reasoning'),
                    context=issue.get('context')
                )
    
    def _save_ai_output(self, task_id: int, operation_type: str, 
                       input_text: str, result: Dict[str, Any]):
        """ä¿å­˜AIè¾“å‡ºç»“æœ"""
        self.ai_output_repo.create(
            task_id=task_id,
            operation_type=operation_type,
            input_text=input_text,
            raw_output=result.get('raw_output', json.dumps(result)),
            parsed_output=result.get('data'),
            status=result.get('status', 'success'),
            error_message=result.get('error_message'),
            tokens_used=result.get('tokens_used'),
            processing_time=result.get('processing_time')
        )
    
    async def _log(self, task_id: int, level: str, message: str, stage: str = None, progress: int = None):
        """è®°å½•æ—¥å¿—å¹¶å®æ—¶æ¨é€"""
        # è¿‡æ»¤ç©ºæ¶ˆæ¯ï¼Œé¿å…äº§ç”Ÿæ— ç”¨çš„æ—¥å¿—è®°å½•
        if not message or not str(message).strip():
            return
            
        # ä¿å­˜åˆ°æ•°æ®åº“
        log = TaskLog(
            task_id=task_id,
            level=level,
            message=str(message).strip(),
            stage=stage,
            progress=progress
        )
        self.db.add(log)
        self.db.commit()
        
        # å®æ—¶æ¨é€
        await manager.send_log(task_id, level, message, stage, progress)