"""æ–‡æ¡£é¢„å¤„ç†æœåŠ¡ - è´Ÿè´£ç« èŠ‚æå–å’Œæ–‡æ¡£ç»“æ„åˆ†æ"""
import json
import re
import time
import logging
import asyncio
from typing import List, Dict, Optional, Callable, Any
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
try:
    from langchain_core.output_parsers import PydanticOutputParser
except ImportError:
    from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field
from sqlalchemy.orm import Session

from app.services.prompt_loader import prompt_loader
from app.models.ai_output import AIOutput


# å®šä¹‰æ–‡æ¡£ç« èŠ‚æ¨¡å‹
class DocumentSection(BaseModel):
    """æ–‡æ¡£ç« èŠ‚"""
    section_title: str = Field(description="ç« èŠ‚æ ‡é¢˜")
    content: str = Field(description="ç« èŠ‚å†…å®¹")
    level: int = Field(description="ç« èŠ‚å±‚çº§ï¼Œ1ä¸ºä¸€çº§æ ‡é¢˜ï¼Œ2ä¸ºäºŒçº§æ ‡é¢˜ç­‰")


class DocumentStructure(BaseModel):
    """æ–‡æ¡£ç»“æ„"""
    sections: List[DocumentSection] = Field(description="æ–‡æ¡£ç« èŠ‚åˆ—è¡¨")


class DocumentProcessor:
    """æ–‡æ¡£é¢„å¤„ç†æœåŠ¡ - ä¸“é—¨è´Ÿè´£æ–‡æ¡£ç»“æ„åˆ†æå’Œç« èŠ‚æå–"""
    
    def __init__(self, model_config: Dict, db_session: Optional[Session] = None):
        """
        åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨
        
        Args:
            model_config: AIæ¨¡å‹é…ç½®
            db_session: æ•°æ®åº“ä¼šè¯
        """
        self.db = db_session
        self.model_config = model_config
        
        # åˆå§‹åŒ–æ—¥å¿—
        self.logger = logging.getLogger(f"document_processor.{id(self)}")
        self.logger.setLevel(logging.INFO)
        
        # ç¡®ä¿æ—¥å¿—èƒ½è¾“å‡ºåˆ°æ§åˆ¶å°
        if not self.logger.handlers:
            console_handler = logging.StreamHandler()
            console_handler.setLevel(logging.INFO)
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            console_handler.setFormatter(formatter)
            self.logger.addHandler(console_handler)
        
        # ä»é…ç½®ä¸­æå–å‚æ•° - å…¼å®¹ä¸¤ç§é…ç½®æ ¼å¼
        if 'config' in model_config:
            # æ–°æ ¼å¼ï¼šmodel_configåŒ…å«providerå’Œconfig
            config = model_config['config']
            self.provider = model_config.get('provider', 'openai')
        else:
            # æ—§æ ¼å¼ï¼šmodel_configç›´æ¥åŒ…å«é…ç½®
            config = model_config
            self.provider = model_config.get('provider', 'openai')
        
        self.api_key = config.get('api_key')
        self.api_base = config.get('base_url')
        self.model_name = config.get('model')
        self.temperature = config.get('temperature', 0.3)
        self.max_tokens = config.get('max_tokens', 8000)
        self.timeout = config.get('timeout', 60)
        self.max_retries = config.get('max_retries', 3)
        
        # åˆ†å—é…ç½® - åŸºäºæ¨¡å‹çš„max_tokenså’Œreserved_tokens
        self.reserved_tokens = config.get('reserved_tokens', 2000)
        self.chunk_overlap = 200  # åˆ†å—é‡å å­—ç¬¦æ•°
        # è®¡ç®—å¯ç”¨äºæ–‡æ¡£å†…å®¹çš„å­—ç¬¦æ•°ï¼ˆ1ä¸ªtokençº¦ç­‰äº4ä¸ªå­—ç¬¦ï¼‰
        self.available_tokens = self.max_tokens - self.reserved_tokens
        self.max_chunk_chars = self.available_tokens * 4
        
        # æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æ­£ç¡®è·å–
        if not self.api_key:
            self.logger.error(f"âŒ æœªæ‰¾åˆ°APIå¯†é’¥ï¼Œæ¨¡å‹é…ç½®: {model_config}")
            raise ValueError(f"æœªæ‰¾åˆ°APIå¯†é’¥ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒå˜é‡å’Œé…ç½®æ–‡ä»¶")
        
        self.logger.info(f"ğŸ“š æ–‡æ¡£å¤„ç†å™¨åˆå§‹åŒ–: Provider={self.provider}, Model={self.model_name}")
        self.logger.info(f"ğŸ”‘ APIå¯†é’¥çŠ¶æ€: {'å·²é…ç½®' if self.api_key else 'æœªé…ç½®'} (å‰6ä½: {self.api_key[:6]}...)")
        
        try:
            # åˆå§‹åŒ–ChatOpenAIæ¨¡å‹ - æ”¯æŒå¤šç§å…¼å®¹OpenAI APIçš„æä¾›å•†
            self.model = ChatOpenAI(
                api_key=self.api_key,
                base_url=self.api_base,
                model=self.model_name,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                request_timeout=self.timeout,
                max_retries=self.max_retries
            )
            
            # åˆå§‹åŒ–è§£æå™¨
            self.structure_parser = PydanticOutputParser(pydantic_object=DocumentStructure)
            self.logger.info("âœ… æ–‡æ¡£å¤„ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            self.logger.error(f"âŒ æ–‡æ¡£å¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    async def preprocess_document(
        self, 
        text: str, 
        task_id: Optional[int] = None,
        progress_callback: Optional[Callable] = None
    ) -> List[Dict]:
        """
        é¢„å¤„ç†æ–‡æ¡£ï¼šç« èŠ‚åˆ†å‰²å’Œå†…å®¹æ•´ç† - é€šè¿‡AIä¸€æ¬¡æ€§å®Œæˆ
        
        Args:
            text: æ–‡æ¡£æ–‡æœ¬å†…å®¹
            task_id: ä»»åŠ¡ID
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            
        Returns:
            ç« èŠ‚åˆ—è¡¨
        """
        self.logger.info("ğŸ“ å¼€å§‹æ–‡æ¡£é¢„å¤„ç†...")
        start_time = time.time()
        
        if progress_callback:
            await progress_callback("å¼€å§‹åˆ†ææ–‡æ¡£ç»“æ„...", 5)
        
        try:
            # ä»æ¨¡æ¿åŠ è½½æç¤ºè¯
            system_prompt = prompt_loader.get_system_prompt('document_preprocess')
            
            # åˆ†æ‰¹å¤„ç†å¤§æ–‡æ¡£ï¼šæ¯æ‰¹è°ƒç”¨AIè¿›è¡Œç« èŠ‚æ‹†åˆ†
            chunks = self._split_document_intelligently(text)
            
            all_sections = []
            total_chunks = len(chunks)
            
            self.logger.info(f"ğŸ“š å¼€å§‹åˆ†æ‰¹å¤„ç†æ–‡æ¡£ï¼Œå…±{total_chunks}ä¸ªæ‰¹æ¬¡")
            
            # ç¬¬ä¸€é˜¶æ®µï¼šåˆ†æ‰¹AIæ‹†åˆ†ç« èŠ‚ (10%-16%çš„è¿›åº¦)
            for chunk_idx, chunk in enumerate(chunks):
                batch_progress = 10 + (chunk_idx / total_chunks) * 6
                if progress_callback:
                    await progress_callback(f"AIæ‹†åˆ†ç¬¬{chunk_idx + 1}/{total_chunks}æ‰¹æ¬¡çš„ç« èŠ‚...", int(batch_progress))
                
                self.logger.info(f"ğŸ¤– ç¬¬{chunk_idx + 1}/{total_chunks}æ‰¹æ¬¡ï¼šè°ƒç”¨AIæ‹†åˆ†ç« èŠ‚ï¼ˆ{len(chunk)}å­—ç¬¦ï¼‰")
                
                # æ„å»ºç”¨æˆ·æç¤º
                self.logger.debug(f"ğŸ”§ ç¬¬{chunk_idx + 1}æ‰¹æ¬¡ï¼šæ„å»ºAIæç¤º")
                user_prompt = prompt_loader.get_user_prompt(
                    'document_preprocess',
                    format_instructions=self.structure_parser.get_format_instructions(),
                    document_content=chunk
                )
                
                self.logger.debug(f"ğŸ“ ç¬¬{chunk_idx + 1}æ‰¹æ¬¡ï¼šæç¤ºé•¿åº¦ - ç³»ç»Ÿæç¤º: {len(system_prompt)}å­—ç¬¦, ç”¨æˆ·æç¤º: {len(user_prompt)}å­—ç¬¦")
                
                # åˆ›å»ºæ¶ˆæ¯
                messages = [
                    SystemMessage(content=system_prompt),
                    HumanMessage(content=user_prompt)
                ]
                
                # è°ƒç”¨AIæ¨¡å‹å¤„ç†å•ä¸ªæ‰¹æ¬¡
                self.logger.debug(f"ğŸ“¤ ç¬¬{chunk_idx + 1}æ‰¹æ¬¡ï¼šå‘é€è¯·æ±‚åˆ°AIæ¨¡å‹")
                batch_start_time = time.time()
                response = await self._call_ai_model(messages)
                batch_time = time.time() - batch_start_time
                
                self.logger.info(f"ğŸ“¥ ç¬¬{chunk_idx + 1}æ‰¹æ¬¡ï¼šæ”¶åˆ°AIå“åº” - è€—æ—¶: {batch_time:.2f}s, å“åº”é•¿åº¦: {len(response.content)}å­—ç¬¦")
                
                # è§£æè¿™ä¸ªæ‰¹æ¬¡çš„AIå“åº”ç»“æœ
                self.logger.debug(f"ğŸ” ç¬¬{chunk_idx + 1}æ‰¹æ¬¡ï¼šå¼€å§‹è§£æAIå“åº”")
                chunk_sections = self._parse_response(response.content, f"batch_{chunk_idx + 1}")
                if chunk_sections:
                    self.logger.info(f"âœ… ç¬¬{chunk_idx + 1}æ‰¹æ¬¡å®Œæˆï¼šè¯†åˆ«åˆ°{len(chunk_sections)}ä¸ªç« èŠ‚ (è€—æ—¶: {batch_time:.2f}s)")
                    all_sections.extend(chunk_sections)
                else:
                    self.logger.warning(f"âš ï¸ ç¬¬{chunk_idx + 1}æ‰¹æ¬¡æœªè¯†åˆ«åˆ°æœ‰æ•ˆç« èŠ‚")
            
            # ç¬¬äºŒé˜¶æ®µï¼šå¤§æ¨¡å‹è´¨é‡æ£€æŸ¥å’Œä¼˜åŒ– (16%-18%çš„è¿›åº¦)
            if progress_callback:
                await progress_callback(f"å¤§æ¨¡å‹è¯„ä¼°{len(all_sections)}ä¸ªç« èŠ‚çš„åˆ†å‰²è´¨é‡...", 16)
            
            self.logger.info(f"ğŸ¤– å¼€å§‹å¤§æ¨¡å‹è´¨é‡æ£€æŸ¥{total_chunks}ä¸ªæ‰¹æ¬¡å¾—åˆ°çš„{len(all_sections)}ä¸ªç« èŠ‚")
            optimized_sections = await self._ai_optimize_sections(all_sections, text)
            self.logger.info(f"âœ… AIä¼˜åŒ–å®Œæˆï¼š{len(all_sections)} -> {len(optimized_sections)}ä¸ªä¼˜åŒ–ç« èŠ‚")
            
            # ç¬¬ä¸‰é˜¶æ®µï¼šéªŒè¯å’Œæ¸…ç†ç« èŠ‚ (18%-20%çš„è¿›åº¦)
            if progress_callback:
                await progress_callback(f"éªŒè¯{len(optimized_sections)}ä¸ªä¼˜åŒ–ç« èŠ‚...", 18)
            
            self.logger.info(f"ğŸ” å¼€å§‹éªŒè¯ä¼˜åŒ–åçš„{len(optimized_sections)}ä¸ªç« èŠ‚")
            merged_sections = self.validate_sections(optimized_sections)
            self.logger.info(f"âœ… ç« èŠ‚éªŒè¯å®Œæˆï¼š{len(optimized_sections)} -> {len(merged_sections)}ä¸ªæœ‰æ•ˆç« èŠ‚")
            
            processing_time = time.time() - start_time
            self.logger.info(f"ğŸ“¥ æ–‡æ¡£é¢„å¤„ç†å®Œæˆï¼Œå…±å¤„ç†{total_chunks}ä¸ªç‰‡æ®µï¼Œå¾—åˆ°{len(merged_sections)}ä¸ªç« èŠ‚ (è€—æ—¶: {processing_time:.2f}s)")

            # ä¿å­˜åˆå¹¶åçš„ç»“æœåˆ°æ•°æ®åº“
            if self.db and task_id:
                ai_output = AIOutput(
                    task_id=task_id,
                    operation_type="preprocess",
                    input_text=text[:1000],  # ä¿å­˜å‰1000å­—ç¬¦ä½œä¸ºæ ·æœ¬
                    raw_output=json.dumps({"sections": merged_sections}, ensure_ascii=False),
                    parsed_output={"sections": merged_sections},
                    processing_time=processing_time,
                    status="success"
                )
                self.db.add(ai_output)
                self.db.commit()
            
            if progress_callback:
                await progress_callback(f"æ–‡æ¡£è§£æå®Œæˆï¼Œè¯†åˆ«åˆ° {len(merged_sections)} ä¸ªç« èŠ‚", 20)
            
            self.logger.info(f"âœ… æ–‡æ¡£é¢„å¤„ç†å®Œæˆï¼Œè¯†åˆ«åˆ° {len(merged_sections)} ä¸ªç« èŠ‚")
            return merged_sections
                
        except Exception as e:
            self.logger.error(f"âŒ æ–‡æ¡£é¢„å¤„ç†å¤±è´¥: {str(e)}")
            processing_time = time.time() - start_time
            
            # ä¿å­˜é”™è¯¯ä¿¡æ¯åˆ°æ•°æ®åº“
            if self.db and task_id:
                ai_output = AIOutput(
                    task_id=task_id,
                    operation_type="preprocess",
                    input_text=text[:10000],
                    raw_output="",
                    status="failed",
                    error_message=str(e),
                    processing_time=processing_time
                )
                self.db.add(ai_output)
                self.db.commit()
            
            if progress_callback:
                await progress_callback("æ–‡æ¡£é¢„å¤„ç†å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ–‡æ¡£", 20)
            
            # è¿”å›åŸå§‹æ–‡æœ¬ä½œä¸ºå•ä¸€ç« èŠ‚
            return [{"section_title": "æ–‡æ¡£å†…å®¹", "content": text, "level": 1}]
    
    def validate_sections(self, sections: List[Dict]) -> List[Dict]:
        """
        éªŒè¯å’Œè¿‡æ»¤ç« èŠ‚
        
        Args:
            sections: ç« èŠ‚åˆ—è¡¨
            
        Returns:
            æœ‰æ•ˆçš„ç« èŠ‚åˆ—è¡¨
        """
        valid_sections = []
        
        for section in sections:
            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            if not isinstance(section, dict):
                self.logger.warning("âš ï¸ è·³è¿‡éå­—å…¸ç±»å‹çš„ç« èŠ‚")
                continue
                
            if 'content' not in section or not section['content']:
                self.logger.warning("âš ï¸ è·³è¿‡æ²¡æœ‰å†…å®¹çš„ç« èŠ‚")
                continue
                
            # æ£€æŸ¥å†…å®¹é•¿åº¦
            content = section['content']
            if len(content.strip()) < 20:
                self.logger.warning(f"âš ï¸ è·³è¿‡å†…å®¹å¤ªçŸ­çš„ç« èŠ‚: {section.get('section_title', 'æœªçŸ¥')}")
                continue
            
            # è®¾ç½®é»˜è®¤å€¼
            if 'section_title' not in section:
                section['section_title'] = 'æœªå‘½åç« èŠ‚'
            if 'level' not in section:
                section['level'] = 1
                
            valid_sections.append(section)
        
        self.logger.info(f"ğŸ“Š ç« èŠ‚éªŒè¯å®Œæˆ: {len(sections)} -> {len(valid_sections)}")
        return valid_sections
    
    def _split_document_intelligently(self, text: str) -> List[str]:
        """
        åŸºäºæ¨¡å‹tokené™åˆ¶åˆ†å‰²æ–‡æ¡£
        
        Args:
            text: åŸå§‹æ–‡æ¡£æ–‡æœ¬
            
        Returns:
            åˆ†å‰²åçš„æ‰¹æ¬¡åˆ—è¡¨ï¼Œæ¯æ‰¹æ¬¡é€‚åˆAIå¤„ç†
        """
        # ä½¿ç”¨æ¨¡å‹é…ç½®è®¡ç®—æ‰¹æ¬¡å¤§å°
        # max_tokens - reserved_tokens = available_tokens
        # available_tokens * 4 = available_chars (1 token â‰ˆ 4 chars)
        batch_chars = self.max_chunk_chars
        min_batch_chars = max(1000, batch_chars // 10)  # æœ€å°æ‰¹æ¬¡ä¸å°‘äº1000å­—ç¬¦
        
        self.logger.info(f"ğŸ“ åˆ†å‰²å‚æ•°ï¼šæ–‡æ¡£{len(text)}å­—ç¬¦ï¼Œæ‰¹æ¬¡å¤§å°={batch_chars}å­—ç¬¦ (åŸºäº{self.available_tokens} tokens)")
        
        if len(text) <= batch_chars:
            self.logger.info(f"ğŸ“„ æ–‡æ¡£è¾ƒå°({len(text)}å­—ç¬¦ <= {batch_chars})ï¼Œå•æ‰¹æ¬¡å¤„ç†")
            return [text]
        
        batches = []
        current_batch = ""
        
        # ç¬¬ä¸€æ­¥ï¼šå°è¯•æŒ‰ç« èŠ‚åˆ†å‰²ï¼ˆæ ‡é¢˜æ¨¡å¼ï¼š# ã€## ã€### ç­‰ï¼‰
        sections = re.split(r'\n(?=#{1,6}\s)', text)
        self.logger.info(f"ğŸ“– æ–‡æ¡£æŒ‰ç« èŠ‚åˆ†å‰²ä¸º{len(sections)}ä¸ªéƒ¨åˆ†")
        
        # å¦‚æœç« èŠ‚å¾ˆå°‘ä½†æ–‡æ¡£è¾ƒé•¿ï¼ŒæŒ‰æ®µè½åˆ†å‰²
        if len(sections) <= 2 and len(text) > batch_chars:
            self.logger.info(f"ğŸ“„ ç« èŠ‚è¾ƒå°‘({len(sections)}ä¸ª)ä¸”æ–‡æ¡£è¾ƒé•¿ï¼ŒæŒ‰æ®µè½åˆ†å‰²")
            return self._split_by_paragraphs(text, batch_chars, min_batch_chars)
        
        # æŒ‰ç« èŠ‚ç»„è£…æ‰¹æ¬¡
        for section in sections:
            # å¦‚æœå½“å‰æ‰¹æ¬¡åŠ ä¸Šè¿™ä¸ªç« èŠ‚ä¼šè¶…è¿‡é™åˆ¶
            if current_batch and len(current_batch + section) > batch_chars:
                # ä¿å­˜å½“å‰æ‰¹æ¬¡
                if current_batch.strip():
                    batches.append(current_batch.strip())
                    self.logger.debug(f"ğŸ“¦ å®Œæˆæ‰¹æ¬¡{len(batches)}ï¼š{len(current_batch)}å­—ç¬¦")
                current_batch = section
            else:
                # æ·»åŠ åˆ°å½“å‰æ‰¹æ¬¡
                current_batch += "\n" + section if current_batch else section
            
            # å¦‚æœå•ä¸ªç« èŠ‚è¿‡é•¿ï¼ŒæŒ‰æ®µè½åˆ†å‰²
            if len(section) > batch_chars:
                self.logger.warning(f"âš ï¸ ç« èŠ‚è¿‡é•¿({len(section)}å­—ç¬¦)ï¼ŒæŒ‰æ®µè½åˆ†å‰²")
                
                # å¦‚æœå½“å‰æ‰¹æ¬¡ä¸æ˜¯è¯¥ç« èŠ‚æœ¬èº«ï¼Œå…ˆä¿å­˜ä¹‹å‰çš„å†…å®¹
                if current_batch != section and current_batch.strip():
                    batches.append(current_batch.replace(section, "").strip())
                
                # åˆ†å‰²è¶…é•¿ç« èŠ‚
                section_batches = self._split_by_paragraphs(section, batch_chars, min_batch_chars)
                batches.extend(section_batches)
                current_batch = ""
        
        # æ·»åŠ æœ€åä¸€ä¸ªæ‰¹æ¬¡
        if current_batch.strip():
            batches.append(current_batch.strip())
        
        self.logger.info(f"ğŸ“Š æ–‡æ¡£åˆ†å‰²å®Œæˆï¼š{len(batches)}ä¸ªæ‰¹æ¬¡ï¼Œå¹³å‡{sum(len(b) for b in batches) // len(batches) if batches else 0}å­—ç¬¦/æ‰¹æ¬¡")
        
        return batches
    
    def _parse_response(self, content: str, chunk_id: str) -> List[Dict]:
        """
        è§£æAIå“åº”å†…å®¹
        
        Args:
            content: AIè¿”å›çš„åŸå§‹å†…å®¹
            chunk_id: åˆ†å—æ ‡è¯†
            
        Returns:
            è§£æå‡ºçš„ç« èŠ‚åˆ—è¡¨
        """
        try:
            # æŸ¥æ‰¾JSONå†…å®¹
            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                result = json.loads(json_str)
                sections = result.get('sections', [])
                self.logger.info(f"âœ… {chunk_id} è§£ææˆåŠŸï¼Œå¾—åˆ° {len(sections)} ä¸ªç« èŠ‚")
                return sections
            else:
                self.logger.warning(f"âš ï¸ {chunk_id} å“åº”ä¸­æœªæ‰¾åˆ°JSONæ ¼å¼ï¼Œå°è¯•æ–‡æœ¬è§£æ")
                return self._parse_text_fallback(content)
        except json.JSONDecodeError as e:
            self.logger.error(f"âŒ {chunk_id} JSONè§£æå¤±è´¥: {str(e)}ï¼Œå°è¯•æ–‡æœ¬è§£æ")
            return self._parse_text_fallback(content)
        except Exception as e:
            self.logger.error(f"âŒ {chunk_id} è§£æå®Œå…¨å¤±è´¥: {str(e)}")
            return []
    
    def _parse_text_fallback(self, content: str) -> List[Dict]:
        """
        å½“JSONè§£æå¤±è´¥æ—¶çš„æ–‡æœ¬è§£æåå¤‡æ–¹æ¡ˆ
        
        Args:
            content: AIå“åº”å†…å®¹
            
        Returns:
            è§£æå‡ºçš„ç« èŠ‚åˆ—è¡¨
        """
        sections = []
        # ç®€å•çš„åŸºäºæ ‡é¢˜çš„æ–‡æœ¬åˆ†å‰²
        parts = re.split(r'\n(?=#{1,6}\s)', content)
        
        for i, part in enumerate(parts):
            if part.strip():
                title_match = re.match(r'^(#{1,6})\s*(.+)', part.strip())
                if title_match:
                    level = len(title_match.group(1))
                    title = title_match.group(2)
                    content_text = part[len(title_match.group(0)):].strip()
                else:
                    level = 1
                    title = f"ç« èŠ‚ {i+1}"
                    content_text = part.strip()
                
                if len(content_text) > 20:  # è¿‡æ»¤å¤ªçŸ­çš„å†…å®¹
                    sections.append({
                        "section_title": title,
                        "content": content_text,
                        "level": level
                    })
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆç« èŠ‚ï¼Œè¿”å›é»˜è®¤ç« èŠ‚
        if not sections:
            sections.append({
                "section_title": "æ–‡æ¡£å†…å®¹",
                "content": content,
                "level": 1
            })
        
        return sections
    
    def _split_by_paragraphs(self, text: str, batch_chars: int, min_batch_chars: int) -> List[str]:
        """
        æŒ‰æ®µè½åˆ†å‰²æ–‡æœ¬ï¼Œå¢å¼ºåˆ†å‰²ç­–ç•¥
        
        Args:
            text: è¦åˆ†å‰²çš„æ–‡æœ¬
            batch_chars: æ‰¹æ¬¡å¤§å°é™åˆ¶
            min_batch_chars: æœ€å°æ‰¹æ¬¡å¤§å°
            
        Returns:
            åˆ†å‰²åçš„æ‰¹æ¬¡åˆ—è¡¨
        """
        batches = []
        current_batch = ""
        
        # å°è¯•å¤šç§åˆ†å‰²ç­–ç•¥
        # 1. é¦–å…ˆæŒ‰åŒæ¢è¡Œç¬¦åˆ†å‰²
        paragraphs = text.split('\n\n')
        self.logger.info(f"ğŸ“„ æŒ‰æ®µè½(\\n\\n)åˆ†å‰²å¾—åˆ°{len(paragraphs)}ä¸ªæ®µè½")
        
        # å¦‚æœæ®µè½æ•°é‡å¤ªå°‘ï¼Œå°è¯•å•æ¢è¡Œç¬¦åˆ†å‰²
        if len(paragraphs) <= 3 and len(text) > batch_chars:
            paragraphs = text.split('\n')
            self.logger.info(f"ğŸ“„ æŒ‰è¡Œ(\\n)åˆ†å‰²å¾—åˆ°{len(paragraphs)}ä¸ªè¡Œ")
            
            # å¦‚æœè¿˜æ˜¯å¤ªå°‘ï¼ŒæŒ‰å›ºå®šé•¿åº¦å¼ºåˆ¶åˆ†å‰²
            if len(paragraphs) <= 10:
                return self._force_split_by_length(text, batch_chars, min_batch_chars)
        
        for i, paragraph in enumerate(paragraphs):
            paragraph = paragraph.strip()
            if not paragraph:  # è·³è¿‡ç©ºæ®µè½
                continue
                
            # å¦‚æœå•ä¸ªæ®µè½å°±è¶…è¿‡æ‰¹æ¬¡å¤§å°ï¼Œè¿›ä¸€æ­¥åˆ†å‰²
            if len(paragraph) > batch_chars:
                # ä¿å­˜å½“å‰æ‰¹æ¬¡
                if current_batch.strip():
                    batches.append(current_batch.strip())
                    current_batch = ""
                
                # æŒ‰å¥å·åˆ†å‰²è¶…é•¿æ®µè½
                sentences = paragraph.split('ã€‚')
                for sentence in sentences:
                    if sentence.strip():
                        sentence_with_period = sentence.strip() + 'ã€‚' if not sentence.endswith('ã€‚') else sentence.strip()
                        
                        if current_batch and len(current_batch + sentence_with_period) > batch_chars:
                            if len(current_batch) >= min_batch_chars:
                                batches.append(current_batch.strip())
                                current_batch = sentence_with_period
                            else:
                                current_batch += sentence_with_period
                        else:
                            current_batch += sentence_with_period if not current_batch else '\n' + sentence_with_period
                continue
            
            # æ­£å¸¸æ®µè½å¤„ç†
            if current_batch and len(current_batch + '\n\n' + paragraph) > batch_chars:
                if len(current_batch) >= min_batch_chars:
                    batches.append(current_batch.strip())
                    self.logger.debug(f"ğŸ“¦ å®Œæˆæ®µè½æ‰¹æ¬¡{len(batches)}ï¼š{len(current_batch)}å­—ç¬¦")
                    current_batch = paragraph
                else:
                    current_batch += '\n\n' + paragraph
            else:
                current_batch += '\n\n' + paragraph if current_batch else paragraph
        
        # æ·»åŠ æœ€åä¸€ä¸ªæ‰¹æ¬¡
        if current_batch.strip():
            batches.append(current_batch.strip())
        
        # éªŒè¯åˆ†å‰²æ•ˆæœ
        total_chars = sum(len(batch) for batch in batches)
        self.logger.info(f"ğŸ“Š æ®µè½åˆ†å‰²å®Œæˆï¼š{len(batches)}ä¸ªæ‰¹æ¬¡ï¼Œæ€»è®¡{total_chars}å­—ç¬¦ï¼Œå¹³å‡{total_chars // len(batches) if batches else 0}å­—ç¬¦/æ‰¹æ¬¡")
        
        return batches
    
    def _force_split_by_length(self, text: str, batch_chars: int, min_batch_chars: int) -> List[str]:
        """
        å¼ºåˆ¶æŒ‰é•¿åº¦åˆ†å‰²æ–‡æ¡£
        
        Args:
            text: æ–‡æ¡£æ–‡æœ¬
            batch_chars: ç›®æ ‡æ‰¹æ¬¡å¤§å°
            min_batch_chars: æœ€å°æ‰¹æ¬¡å¤§å°
            
        Returns:
            åˆ†å‰²åçš„æ‰¹æ¬¡åˆ—è¡¨
        """
        batches = []
        start = 0
        
        while start < len(text):
            # è®¡ç®—è¿™ä¸ªæ‰¹æ¬¡çš„ç»“æŸä½ç½®
            end = start + batch_chars
            
            # å¦‚æœä¸æ˜¯æœ€åä¸€æ‰¹æ¬¡ï¼Œå°è¯•åœ¨åˆé€‚ä½ç½®åˆ†å‰²ï¼ˆé¿å…åœ¨å•è¯ä¸­é—´ï¼‰
            if end < len(text):
                # å‘åæŸ¥æ‰¾åˆé€‚çš„åˆ†å‰²ç‚¹ï¼ˆæ¢è¡Œç¬¦ã€å¥å·ã€ç©ºæ ¼ï¼‰
                search_start = max(start + min_batch_chars, end - 500)  # åœ¨ç›®æ ‡ä½ç½®å‰500å­—ç¬¦å†…æŸ¥æ‰¾
                for split_char in ['\n\n', '\n', 'ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼Œ', ' ']:
                    split_pos = text.rfind(split_char, search_start, end)
                    if split_pos > search_start:
                        end = split_pos + len(split_char)
                        break
            
            # æå–è¿™ä¸ªæ‰¹æ¬¡
            batch = text[start:end].strip()
            if batch and len(batch) >= min_batch_chars:
                batches.append(batch)
                self.logger.debug(f"ğŸ“¦ å¼ºåˆ¶åˆ†å‰²æ‰¹æ¬¡{len(batches)}ï¼š{len(batch)}å­—ç¬¦")
            
            start = end
        
        self.logger.info(f"ğŸ”¨ å¼ºåˆ¶åˆ†å‰²å®Œæˆï¼š{len(batches)}ä¸ªæ‰¹æ¬¡")
        return batches
    
    async def _ai_optimize_sections(self, sections: List[Dict], original_text: str) -> List[Dict]:
        """
        ä½¿ç”¨å¤§æ¨¡å‹è¯„ä¼°å’Œä¼˜åŒ–ç« èŠ‚åˆ†å‰²è´¨é‡
        
        Args:
            sections: åˆæ­¥è¯†åˆ«çš„ç« èŠ‚åˆ—è¡¨
            original_text: åŸå§‹æ–‡æ¡£æ–‡æœ¬
            
        Returns:
            ä¼˜åŒ–åçš„ç« èŠ‚åˆ—è¡¨
        """
        if not sections:
            return sections
        
        try:
            # æ„å»ºè´¨é‡è¯„ä¼°æç¤º
            quality_assessment = await self._assess_section_quality(sections, original_text)
            
            if quality_assessment.get('needs_optimization', False):
                self.logger.info(f"ğŸ”„ å¤§æ¨¡å‹å»ºè®®ä¼˜åŒ–ç« èŠ‚åˆ†å‰²ï¼š{quality_assessment.get('reason', 'æœªçŸ¥åŸå› ')}")
                
                # æ ¹æ®å¤§æ¨¡å‹å»ºè®®è¿›è¡Œä¼˜åŒ–
                optimized_sections = await self._apply_ai_optimization(sections, quality_assessment)
                
                return optimized_sections
            else:
                self.logger.info(f"âœ… å¤§æ¨¡å‹ç¡®è®¤ç« èŠ‚åˆ†å‰²è´¨é‡è‰¯å¥½")
                return sections
                
        except Exception as e:
            self.logger.error(f"âŒ AIä¼˜åŒ–è¿‡ç¨‹å¤±è´¥: {str(e)}ï¼Œä½¿ç”¨åŸå§‹ç« èŠ‚")
            return sections
    
    async def _assess_section_quality(self, sections: List[Dict], original_text: str) -> Dict:
        """
        è®©å¤§æ¨¡å‹è¯„ä¼°ç« èŠ‚åˆ†å‰²è´¨é‡
        
        Args:
            sections: ç« èŠ‚åˆ—è¡¨
            original_text: åŸå§‹æ–‡æ¡£æ–‡æœ¬
            
        Returns:
            è´¨é‡è¯„ä¼°ç»“æœ
        """
        # æ„å»ºç« èŠ‚æ‘˜è¦ä¿¡æ¯
        section_summaries = []
        for i, section in enumerate(sections):
            title = section.get('section_title', f'ç« èŠ‚{i+1}')
            content_preview = section.get('content', '')[:200] + "..." if len(section.get('content', '')) > 200 else section.get('content', '')
            content_length = len(section.get('content', ''))
            level = section.get('level', 1)
            
            section_summaries.append(f"""
ç« èŠ‚ {i+1}: {title} (å±‚çº§: {level}, é•¿åº¦: {content_length}å­—ç¬¦)
å†…å®¹é¢„è§ˆ: {content_preview}
""")
        
        # æ„å»ºè¯„ä¼°æç¤º
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£ç»“æ„åˆ†æä¸“å®¶ã€‚è¯·é€ä¸ªè¯„ä¼°ä»¥ä¸‹ç« èŠ‚çš„å®Œæ•´æ€§å’Œåˆ†å‰²è´¨é‡ï¼š

## ä¸»è¦æ£€æµ‹ç‚¹ï¼š
1. **ç« èŠ‚å®Œæ•´æ€§æ£€æµ‹**ï¼šç« èŠ‚å†…å®¹æ˜¯å¦åœ¨å¥å­ã€æ®µè½æˆ–é€»è¾‘å•å…ƒä¸­é—´è¢«åˆ‡æ–­
2. **è¾¹ç•Œåˆç†æ€§**ï¼šç« èŠ‚å¼€å§‹å’Œç»“æŸä½ç½®æ˜¯å¦ç¬¦åˆæ–‡æ¡£é€»è¾‘ç»“æ„  
3. **å†…å®¹è¿ç»­æ€§**ï¼šæ˜¯å¦æœ‰ç›¸å…³å†…å®¹è¢«ä¸åˆç†åˆ†ç¦»åˆ°ä¸åŒç« èŠ‚
4. **åˆ†å‰²å‡†ç¡®æ€§**ï¼šç« èŠ‚è¾¹ç•Œæ˜¯å¦å‡†ç¡®è¯†åˆ«äº†æ ‡é¢˜å’Œå†…å®¹çš„åˆ†ç•Œçº¿

## å®Œæ•´æ€§åˆ¤æ–­æ ‡å‡†ï¼š
- **å®Œæ•´(complete)**ï¼šç« èŠ‚æœ‰æ˜ç¡®å¼€å¤´å’Œç»“å°¾ï¼Œå†…å®¹é€»è¾‘å®Œæ•´
- **ä¸å®Œæ•´(incomplete)**ï¼šå†…å®¹åœ¨å¥å­/æ®µè½ä¸­é—´æˆªæ–­ï¼Œæˆ–ç¼ºå°‘å¼€å¤´/ç»“å°¾
- **éœ€åˆå¹¶(need_merge)**ï¼šä¸ç›¸é‚»ç« èŠ‚åº”è¯¥åˆå¹¶ä¸ºä¸€ä¸ªé€»è¾‘å•å…ƒ
- **éœ€åˆ†å‰²(need_split)**ï¼šåŒ…å«äº†åº”è¯¥åˆ†å¼€çš„å¤šä¸ªé€»è¾‘å•å…ƒ

è¯·ä»¥JSONæ ¼å¼è¿”å›è¯„ä¼°ç»“æœï¼š
{
  "needs_optimization": true/false,
  "overall_quality": "good/fair/poor",
  "section_completeness": [
    {
      "section_index": 0,
      "completeness_status": "complete/incomplete/need_merge/need_split", 
      "confidence": 0.9,
      "issues": ["å…·ä½“é—®é¢˜æè¿°"],
      "content_boundary_analysis": "å¼€å¤´/ç»“å°¾è¾¹ç•Œåˆ†æ"
    }
  ],
  "recommendations": {
    "merge_pairs": [[ç´¢å¼•1, ç´¢å¼•2]],
    "split_suggestions": [{"section_index": ç´¢å¼•, "reason": "åŸå› "}]
  },
  "reason": "ä¸»è¦é—®é¢˜æ€»ç»“"
}"""
        
        user_prompt = f"""
## æ–‡æ¡£åŸºæœ¬ä¿¡æ¯
- æ–‡æ¡£æ€»é•¿åº¦: {len(original_text)}å­—ç¬¦
- è¯†åˆ«åˆ°çš„ç« èŠ‚æ•°é‡: {len(sections)}

## ç« èŠ‚è¯¦ç»†ä¿¡æ¯
{''.join(section_summaries)}

## ä»»åŠ¡è¦æ±‚
è¯·é€ä¸ªåˆ†ææ¯ä¸ªç« èŠ‚çš„å®Œæ•´æ€§çŠ¶æ€ï¼Œé‡ç‚¹æ£€æŸ¥ï¼š
1. æ¯ä¸ªç« èŠ‚çš„å¼€å¤´æ˜¯å¦æ˜¯è‡ªç„¶çš„å¼€å§‹ç‚¹ï¼ˆå¦‚æ ‡é¢˜ã€æ®µè½å¼€å¤´ï¼‰
2. æ¯ä¸ªç« èŠ‚çš„ç»“å°¾æ˜¯å¦æ˜¯è‡ªç„¶çš„ç»“æŸç‚¹ï¼ˆå¦‚æ®µè½ç»“å°¾ã€å®Œæ•´å¥å­ï¼‰
3. ç« èŠ‚å†…å®¹æ˜¯å¦æ„æˆä¸€ä¸ªå®Œæ•´çš„é€»è¾‘å•å…ƒ
4. æ˜¯å¦æœ‰ç« èŠ‚åº”è¯¥åˆå¹¶æˆ–åˆ†å‰²

è¯·ä¸ºæ¯ä¸ªç« èŠ‚æä¾›æ˜ç¡®çš„å®Œæ•´æ€§æ ‡è®°å’Œå»ºè®®ã€‚
"""
        
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt)
        ]
        
        response = await self._call_ai_model(messages)
        
        # è§£æAIå“åº”
        return self._parse_quality_assessment(response.content)
    
    def _parse_quality_assessment(self, content: str) -> Dict:
        """
        è§£æè´¨é‡è¯„ä¼°å“åº”
        
        Args:
            content: AIå“åº”å†…å®¹
            
        Returns:
            è§£æåçš„è¯„ä¼°ç»“æœ
        """
        try:
            import json
            import re
            
            # æŸ¥æ‰¾JSONå†…å®¹
            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                result = json.loads(json_str)
                self.logger.debug(f"ğŸ” AIè´¨é‡è¯„ä¼°ç»“æœ: {result}")
                return result
            else:
                self.logger.warning("âš ï¸ AIå“åº”ä¸­æœªæ‰¾åˆ°JSONæ ¼å¼çš„è¯„ä¼°ç»“æœ")
                return {'needs_optimization': False, 'overall_quality': 'fair'}
                
        except json.JSONDecodeError as e:
            self.logger.error(f"âŒ è´¨é‡è¯„ä¼°JSONè§£æå¤±è´¥: {str(e)}")
            return {'needs_optimization': False, 'overall_quality': 'fair'}
        except Exception as e:
            self.logger.error(f"âŒ è´¨é‡è¯„ä¼°è§£æå®Œå…¨å¤±è´¥: {str(e)}")
            return {'needs_optimization': False, 'overall_quality': 'fair'}
    
    async def _apply_ai_optimization(self, sections: List[Dict], assessment: Dict) -> List[Dict]:
        """
        æ ¹æ®AIè¯„ä¼°ç»“æœåº”ç”¨ä¼˜åŒ–
        
        Args:
            sections: åŸå§‹ç« èŠ‚åˆ—è¡¨
            assessment: AIè¯„ä¼°ç»“æœ
            
        Returns:
            ä¼˜åŒ–åçš„ç« èŠ‚åˆ—è¡¨
        """
        optimized_sections = sections.copy()
        
        # é¦–å…ˆå¤„ç†ç« èŠ‚å®Œæ•´æ€§æ ‡è®°
        section_completeness = assessment.get('section_completeness', [])
        if section_completeness:
            self.logger.info(f"ğŸ“‹ å¤„ç†{len(section_completeness)}ä¸ªç« èŠ‚çš„å®Œæ•´æ€§æ ‡è®°")
            optimized_sections = self._apply_completeness_fixes(optimized_sections, section_completeness)
        
        # ç„¶ååº”ç”¨å…¶ä»–å»ºè®®
        recommendations = assessment.get('recommendations', {})
        
        # åº”ç”¨åˆå¹¶å»ºè®®
        merge_pairs = recommendations.get('merge_pairs', [])
        if merge_pairs:
            self.logger.info(f"ğŸ“‹ åº”ç”¨AIåˆå¹¶å»ºè®®: {len(merge_pairs)}å¯¹ç« èŠ‚")
            optimized_sections = self._apply_merge_recommendations(optimized_sections, merge_pairs)
        
        return optimized_sections
    
    def _apply_completeness_fixes(self, sections: List[Dict], completeness_data: List[Dict]) -> List[Dict]:
        """
        æ ¹æ®å¤§æ¨¡å‹çš„å®Œæ•´æ€§æ ‡è®°åº”ç”¨ä¿®å¤
        
        Args:
            sections: åŸå§‹ç« èŠ‚åˆ—è¡¨
            completeness_data: å®Œæ•´æ€§æ£€æµ‹ç»“æœ
            
        Returns:
            ä¿®å¤åçš„ç« èŠ‚åˆ—è¡¨
        """
        # ä¸ºæ¯ä¸ªç« èŠ‚æ·»åŠ å®Œæ•´æ€§æ ‡è®°
        for completion_info in completeness_data:
            section_idx = completion_info.get('section_index', -1)
            if 0 <= section_idx < len(sections):
                section = sections[section_idx]
                
                # æ·»åŠ AIåˆ†æç»“æœåˆ°ç« èŠ‚ä¿¡æ¯ä¸­
                section['ai_completeness_status'] = completion_info.get('completeness_status', 'unknown')
                section['ai_confidence'] = completion_info.get('confidence', 0.0)
                section['ai_issues'] = completion_info.get('issues', [])
                section['ai_boundary_analysis'] = completion_info.get('content_boundary_analysis', '')
                
                # è®°å½•æ—¥å¿—
                status = completion_info.get('completeness_status', 'unknown')
                confidence = completion_info.get('confidence', 0.0)
                issues = completion_info.get('issues', [])
                
                if status == 'incomplete':
                    self.logger.warning(f"âš ï¸ ç« èŠ‚{section_idx} '{section.get('section_title', '')}' æ ‡è®°ä¸ºä¸å®Œæ•´ (ç½®ä¿¡åº¦: {confidence:.2f})")
                    if issues:
                        self.logger.warning(f"   é—®é¢˜: {'; '.join(issues)}")
                elif status == 'need_merge':
                    self.logger.info(f"ğŸ”— ç« èŠ‚{section_idx} '{section.get('section_title', '')}' å»ºè®®åˆå¹¶ (ç½®ä¿¡åº¦: {confidence:.2f})")
                elif status == 'need_split':
                    self.logger.info(f"âœ‚ï¸ ç« èŠ‚{section_idx} '{section.get('section_title', '')}' å»ºè®®åˆ†å‰² (ç½®ä¿¡åº¦: {confidence:.2f})")
                elif status == 'complete':
                    self.logger.debug(f"âœ… ç« èŠ‚{section_idx} '{section.get('section_title', '')}' å®Œæ•´æ€§è‰¯å¥½ (ç½®ä¿¡åº¦: {confidence:.2f})")
                
        self.logger.info(f"ğŸ“Š å®Œæ•´æ€§æ ‡è®°ç»Ÿè®¡:")
        status_counts = {}
        for completion_info in completeness_data:
            status = completion_info.get('completeness_status', 'unknown')
            status_counts[status] = status_counts.get(status, 0) + 1
        
        for status, count in status_counts.items():
            self.logger.info(f"   - {status}: {count}ä¸ªç« èŠ‚")
        
        return sections
    
    def _apply_merge_recommendations(self, sections: List[Dict], merge_pairs: List[List[int]]) -> List[Dict]:
        """
        åº”ç”¨åˆå¹¶å»ºè®®
        
        Args:
            sections: ç« èŠ‚åˆ—è¡¨
            merge_pairs: éœ€è¦åˆå¹¶çš„ç« èŠ‚ç´¢å¼•å¯¹
            
        Returns:
            åˆå¹¶åçš„ç« èŠ‚åˆ—è¡¨
        """
        merged_sections = []
        merged_indices = set()
        
        # æŒ‰ç´¢å¼•æ’åºåˆå¹¶å¯¹ï¼Œä»åå¾€å‰å¤„ç†é¿å…ç´¢å¼•å˜åŒ–
        merge_pairs.sort(key=lambda x: max(x), reverse=True)
        
        for pair in merge_pairs:
            if len(pair) >= 2:
                idx1, idx2 = pair[0], pair[1]
                
                # ç¡®ä¿ç´¢å¼•æœ‰æ•ˆä¸”æœªè¢«å¤„ç†
                if (0 <= idx1 < len(sections) and 0 <= idx2 < len(sections) and 
                    idx1 not in merged_indices and idx2 not in merged_indices):
                    
                    # åˆå¹¶å†…å®¹
                    section1 = sections[idx1]
                    section2 = sections[idx2]
                    
                    merged_section = {
                        'section_title': f"{section1.get('section_title', '')} (AIåˆå¹¶)",
                        'content': section1.get('content', '') + '\n\n' + section2.get('content', ''),
                        'level': min(section1.get('level', 1), section2.get('level', 1)),
                        'merged_by_ai': True,
                        'original_indices': [idx1, idx2]
                    }
                    
                    # æ ‡è®°ä¸ºå·²åˆå¹¶
                    merged_indices.update([idx1, idx2])
                    
                    self.logger.debug(f"ğŸ”— AIå»ºè®®åˆå¹¶ç« èŠ‚ {idx1} å’Œ {idx2}")
        
        # æ„å»ºæœ€ç»ˆç« èŠ‚åˆ—è¡¨
        for i, section in enumerate(sections):
            if i not in merged_indices:
                merged_sections.append(section)
        
        # æ·»åŠ åˆå¹¶åçš„ç« èŠ‚
        for pair in merge_pairs:
            if len(pair) >= 2:
                idx1, idx2 = pair[0], pair[1]
                if (0 <= idx1 < len(sections) and 0 <= idx2 < len(sections)):
                    section1 = sections[idx1]
                    section2 = sections[idx2]
                    
                    merged_section = {
                        'section_title': f"{section1.get('section_title', '')} (AIåˆå¹¶)",
                        'content': section1.get('content', '') + '\n\n' + section2.get('content', ''),
                        'level': min(section1.get('level', 1), section2.get('level', 1)),
                        'merged_by_ai': True,
                        'original_indices': [idx1, idx2]
                    }
                    merged_sections.append(merged_section)
        
        return merged_sections
    
    async def _apply_boundary_adjustments(self, sections: List[Dict], adjustments: List[Dict]) -> List[Dict]:
        """
        åº”ç”¨è¾¹ç•Œè°ƒæ•´å»ºè®®
        
        Args:
            sections: ç« èŠ‚åˆ—è¡¨
            adjustments: è¾¹ç•Œè°ƒæ•´å»ºè®®
            
        Returns:
            è°ƒæ•´åçš„ç« èŠ‚åˆ—è¡¨
        """
        # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„è¾¹ç•Œè°ƒæ•´é€»è¾‘
        # ç›®å‰è¿”å›åŸå§‹ç« èŠ‚ï¼Œé¿å…è¿‡åº¦å¤æ‚åŒ–
        self.logger.info(f"ğŸ“‹ æ”¶åˆ°{len(adjustments)}é¡¹è¾¹ç•Œè°ƒæ•´å»ºè®®ï¼Œæš‚æ—¶ä¿æŒåŸæœ‰è¾¹ç•Œ")
        return sections
    
    async def _call_ai_model(self, messages):
        """
        è°ƒç”¨AIæ¨¡å‹ï¼ˆä»…åœ¨æ­¤æ–¹æ³•å†…è¿›è¡Œmockåˆ¤æ–­ï¼‰
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            
        Returns:
            AIæ¨¡å‹å“åº”
        """
        # ç›´æ¥è¿›è¡ŒçœŸå®çš„AIè°ƒç”¨
        return await asyncio.to_thread(self.model.invoke, messages)
    
    async def analyze_document(self, text: str, prompt_type: str = "preprocess") -> Dict[str, Any]:
        """
        ç»Ÿä¸€çš„æ–‡æ¡£åˆ†ææ¥å£ï¼Œå…¼å®¹task_processorçš„è°ƒç”¨
        
        Args:
            text: æ–‡æ¡£æ–‡æœ¬å†…å®¹
            prompt_type: æç¤ºç±»å‹ï¼Œå¯¹äºDocumentProcessorä»…æ”¯æŒ"preprocess"
            
        Returns:
            åˆ†æç»“æœ
        """
        if prompt_type != "preprocess":
            raise ValueError(f"DocumentProcessoråªæ”¯æŒpreprocessç±»å‹ï¼Œæ”¶åˆ°: {prompt_type}")
        
        # è°ƒç”¨é¢„å¤„ç†æ–¹æ³•
        sections = await self.preprocess_document(text)
        
        # æ„å»ºè¿”å›æ ¼å¼ï¼Œå…¼å®¹task_processorçš„æœŸæœ›
        return {
            "status": "success",
            "data": {
                "document_type": "æŠ€æœ¯æ–‡æ¡£",
                "structure": {
                    "total_sections": len(sections),
                    "sections": sections
                }
            },
            "raw_output": json.dumps({"sections": sections}, ensure_ascii=False, indent=2),
            "tokens_used": 100,  # ä¼°ç®—å€¼
            "processing_time": 1.0  # ä¼°ç®—å€¼
        }