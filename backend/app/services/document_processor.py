"""æ–‡æ¡£é¢„å¤„ç†æœåŠ¡ - è´Ÿè´£ç« èŠ‚æå–å’Œæ–‡æ¡£ç»“æ„åˆ†æ"""
import json
import re
import time
import logging
import asyncio
from typing import List, Dict, Optional, Callable, Any
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
try:
    from langchain_core.output_parsers import PydanticOutputParser
except ImportError:
    from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field
from sqlalchemy.orm import Session

from app.services.prompt_loader import prompt_loader
from app.models.ai_output import AIOutput
from app.utils.ai_retry_parser import ai_retry_parser, RetryConfig


# å®šä¹‰æ–‡æ¡£ç« èŠ‚æ¨¡å‹
class DocumentSection(BaseModel):
    """æ–‡æ¡£ç« èŠ‚"""
    section_title: str = Field(description="ç« èŠ‚æ ‡é¢˜")
    content: str = Field(description="ç« èŠ‚å†…å®¹")
    level: int = Field(description="ç« èŠ‚å±‚çº§ï¼Œ1ä¸ºä¸€çº§æ ‡é¢˜ï¼Œ2ä¸ºäºŒçº§æ ‡é¢˜ç­‰")
    completeness_status: str = Field(
        description="ç« èŠ‚å®Œæ•´æ€§çŠ¶æ€ï¼šcomplete(å®Œæ•´)/incomplete(ä¸å®Œæ•´)", 
        default="complete"
    )


class DocumentStructure(BaseModel):
    """æ–‡æ¡£ç»“æ„"""
    sections: List[DocumentSection] = Field(description="æ–‡æ¡£ç« èŠ‚åˆ—è¡¨")


class DocumentProcessor:
    """æ–‡æ¡£é¢„å¤„ç†æœåŠ¡ - ä¸“é—¨è´Ÿè´£æ–‡æ¡£ç»“æ„åˆ†æå’Œç« èŠ‚æå–"""
    
    def __init__(self, model_config: Dict, db_session: Optional[Session] = None):
        """
        åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨
        
        Args:
            model_config: AIæ¨¡å‹é…ç½®
            db_session: æ•°æ®åº“ä¼šè¯
        """
        self.db = db_session
        self.model_config = model_config
        
        # åˆå§‹åŒ–æ—¥å¿—
        self.logger = logging.getLogger(f"document_processor.{id(self)}")
        self.logger.setLevel(logging.INFO)
        
        # ç¡®ä¿æ—¥å¿—èƒ½è¾“å‡ºåˆ°æ§åˆ¶å°
        if not self.logger.handlers:
            console_handler = logging.StreamHandler()
            console_handler.setLevel(logging.INFO)
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            console_handler.setFormatter(formatter)
            self.logger.addHandler(console_handler)
        
        # ä»é…ç½®ä¸­æå–å‚æ•° - å…¼å®¹ä¸¤ç§é…ç½®æ ¼å¼
        if 'config' in model_config:
            # æ–°æ ¼å¼ï¼šmodel_configåŒ…å«providerå’Œconfig
            config = model_config['config']
            self.provider = model_config.get('provider', 'openai')
        else:
            # æ—§æ ¼å¼ï¼šmodel_configç›´æ¥åŒ…å«é…ç½®
            config = model_config
            self.provider = model_config.get('provider', 'openai')
        
        self.api_key = config.get('api_key')
        self.api_base = config.get('base_url')
        self.model_name = config.get('model')
        self.temperature = config.get('temperature', 0.3)
        self.max_tokens = config.get('max_tokens', 8000)
        self.timeout = config.get('timeout', 60)
        self.max_retries = config.get('max_retries', 3)
        
        # åˆ†å—é…ç½® - åŸºäºæ¨¡å‹çš„max_tokenså’Œreserved_tokens
        self.reserved_tokens = config.get('reserved_tokens', 2000)
        # è®¡ç®—å¯ç”¨äºæ–‡æ¡£å†…å®¹çš„å­—ç¬¦æ•°ï¼ˆ1ä¸ªtokençº¦ç­‰äº4ä¸ªå­—ç¬¦ï¼‰
        self.available_tokens = self.max_tokens - self.reserved_tokens
        self.max_chunk_chars = self.available_tokens * 4
        
        # é…ç½®JSONè§£æé‡è¯•
        self.retry_config = RetryConfig(
            max_retries=config.get('json_parse_retries', 3),
            base_delay=config.get('json_retry_delay', 1.0),
            backoff_multiplier=2.0,
            max_delay=10.0
        )
        
        # æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æ­£ç¡®è·å–
        if not self.api_key:
            self.logger.error(f"âŒ æœªæ‰¾åˆ°APIå¯†é’¥ï¼Œæ¨¡å‹é…ç½®: {model_config}")
            raise ValueError(f"æœªæ‰¾åˆ°APIå¯†é’¥ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒå˜é‡å’Œé…ç½®æ–‡ä»¶")
        
        self.logger.info(f"ğŸ“š æ–‡æ¡£å¤„ç†å™¨åˆå§‹åŒ–: Provider={self.provider}, Model={self.model_name}")
        self.logger.info(f"ğŸ”‘ APIå¯†é’¥çŠ¶æ€: {'å·²é…ç½®' if self.api_key else 'æœªé…ç½®'} (å‰6ä½: {self.api_key[:6]}...)")
        self.logger.info(f"ğŸ”„ JSONè§£æé‡è¯•é…ç½®: æœ€å¤§é‡è¯•{self.retry_config.max_retries}æ¬¡, åŸºç¡€å»¶è¿Ÿ{self.retry_config.base_delay}ç§’")
        
        try:
            # åˆå§‹åŒ–ChatOpenAIæ¨¡å‹ - æ”¯æŒå¤šç§å…¼å®¹OpenAI APIçš„æä¾›å•†
            self.model = ChatOpenAI(
                api_key=self.api_key,
                base_url=self.api_base,
                model=self.model_name,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                request_timeout=self.timeout,
                max_retries=self.max_retries
            )
            
            # åˆå§‹åŒ–è§£æå™¨
            self.structure_parser = PydanticOutputParser(pydantic_object=DocumentStructure)
            self.logger.info("âœ… æ–‡æ¡£å¤„ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            self.logger.error(f"âŒ æ–‡æ¡£å¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    async def preprocess_document(
        self, 
        text: str, 
        task_id: Optional[int] = None,
        progress_callback: Optional[Callable] = None
    ) -> List[Dict]:
        """
        é¢„å¤„ç†æ–‡æ¡£ï¼šæŒ‰æ‰¹æ¬¡å¤§å°åˆ†å‰²æ–‡æœ¬ï¼Œç„¶åé€æ‰¹æ¬¡è¿›è¡Œç« èŠ‚æå–
        
        Args:
            text: æ–‡æ¡£æ–‡æœ¬å†…å®¹
            task_id: ä»»åŠ¡ID
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            
        Returns:
            ç« èŠ‚åˆ—è¡¨
        """
        self.logger.info("ğŸ“ å¼€å§‹æ–‡æ¡£é¢„å¤„ç† - æ‰¹æ¬¡åˆ†å‰² + ç« èŠ‚æå–...")
        start_time = time.time()
        
        if progress_callback:
            await progress_callback("å¼€å§‹æ–‡æ¡£åˆ†å‰²...", 5)
        
        try:
            # ç¬¬ä¸€æ­¥ï¼šæŒ‰æ‰¹æ¬¡å¤§å°åˆ†å‰²æ–‡æ¡£
            batches = self._split_text_by_batch_size(text)
            self.logger.info(f"ğŸ“š æ–‡æ¡£åˆ†å‰²å®Œæˆï¼š{len(text)}å­—ç¬¦ -> {len(batches)}ä¸ªæ‰¹æ¬¡")
            
            if progress_callback:
                await progress_callback(f"åˆ†å‰²å®Œæˆï¼Œå¼€å§‹é€æ‰¹æ¬¡åˆ†æ...", 10)
            
            # ç¬¬äºŒæ­¥ï¼šé€æ‰¹æ¬¡è¿›è¡ŒAIç« èŠ‚æå–
            all_sections = []
            for batch_idx, batch_text in enumerate(batches):
                batch_progress = 10 + int((batch_idx / len(batches)) * 80)
                if progress_callback:
                    await progress_callback(f"åˆ†ææ‰¹æ¬¡ {batch_idx + 1}/{len(batches)}...", batch_progress)
                
                batch_sections = await self._extract_sections_from_batch(batch_text, batch_idx)
                all_sections.extend(batch_sections)
                
                self.logger.info(f"ğŸ“„ æ‰¹æ¬¡ {batch_idx + 1} å®Œæˆï¼šè¯†åˆ«åˆ° {len(batch_sections)} ä¸ªç« èŠ‚")
            
            # ç¬¬ä¸‰æ­¥ï¼šéªŒè¯å’Œæ¸…ç†ç« èŠ‚
            if progress_callback:
                await progress_callback(f"éªŒè¯ç« èŠ‚ç»“æ„...", 90)
            
            final_sections = self.validate_sections(all_sections)
            processing_time = time.time() - start_time
            
            self.logger.info(f"âœ… æ–‡æ¡£é¢„å¤„ç†å®Œæˆï¼š{len(all_sections)} -> {len(final_sections)}ä¸ªæœ‰æ•ˆç« èŠ‚ (è€—æ—¶: {processing_time:.2f}s)")

            # ä¿å­˜ç»“æœåˆ°æ•°æ®åº“
            if self.db and task_id:
                ai_output = AIOutput(
                    task_id=task_id,
                    operation_type="preprocess",
                    input_text=text[:1000],  # ä¿å­˜å‰1000å­—ç¬¦ä½œä¸ºæ ·æœ¬
                    raw_output=json.dumps({"sections": final_sections}, ensure_ascii=False),
                    parsed_output={"sections": final_sections},
                    processing_time=processing_time,
                    status="success"
                )
                self.db.add(ai_output)
                self.db.commit()
            
            if progress_callback:
                await progress_callback(f"æ–‡æ¡£é¢„å¤„ç†å®Œæˆï¼Œè¯†åˆ«åˆ° {len(final_sections)} ä¸ªç« èŠ‚", 100)
            
            return final_sections
                
        except Exception as e:
            self.logger.error(f"âŒ æ–‡æ¡£é¢„å¤„ç†å¤±è´¥: {str(e)}")
            processing_time = time.time() - start_time
            
            # ä¿å­˜é”™è¯¯ä¿¡æ¯åˆ°æ•°æ®åº“
            if self.db and task_id:
                ai_output = AIOutput(
                    task_id=task_id,
                    operation_type="preprocess",
                    input_text=text[:1000],
                    raw_output="",
                    status="failed",
                    error_message=str(e),
                    processing_time=processing_time
                )
                self.db.add(ai_output)
                self.db.commit()
            
            if progress_callback:
                await progress_callback("æ–‡æ¡£é¢„å¤„ç†å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ–‡æ¡£", 100)
            
            # è¿”å›åŸå§‹æ–‡æœ¬ä½œä¸ºå•ä¸€ç« èŠ‚
            return [{"section_title": "æ–‡æ¡£å†…å®¹", "content": text, "level": 1}]
    
    def validate_sections(self, sections: List[Dict]) -> List[Dict]:
        """
        éªŒè¯å’Œè¿‡æ»¤ç« èŠ‚
        
        Args:
            sections: ç« èŠ‚åˆ—è¡¨
            
        Returns:
            æœ‰æ•ˆçš„ç« èŠ‚åˆ—è¡¨
        """
        valid_sections = []
        
        for section in sections:
            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            if not isinstance(section, dict):
                self.logger.warning("âš ï¸ è·³è¿‡éå­—å…¸ç±»å‹çš„ç« èŠ‚")
                continue
                
            if 'content' not in section or not section['content']:
                self.logger.warning("âš ï¸ è·³è¿‡æ²¡æœ‰å†…å®¹çš„ç« èŠ‚")
                continue
                
            # æ£€æŸ¥å†…å®¹é•¿åº¦
            content = section['content']
            if len(content.strip()) < 20:
                self.logger.warning(f"âš ï¸ è·³è¿‡å†…å®¹å¤ªçŸ­çš„ç« èŠ‚: {section.get('section_title', 'æœªçŸ¥')}")
                continue
            
            # è®¾ç½®é»˜è®¤å€¼
            if 'section_title' not in section:
                section['section_title'] = 'æœªå‘½åç« èŠ‚'
            if 'level' not in section:
                section['level'] = 1
                
            valid_sections.append(section)
        
        # ä¸ºæ¯ä¸ªç« èŠ‚æ·»åŠ é»˜è®¤å®Œæ•´æ€§çŠ¶æ€ï¼ˆå¦‚æœæœªè®¾ç½®ï¼‰
        for section in valid_sections:
            if 'completeness_status' not in section:
                section['completeness_status'] = 'unknown'
        
        self.logger.info(f"ğŸ“Š ç« èŠ‚éªŒè¯å®Œæˆ: {len(sections)} -> {len(valid_sections)}")
        return valid_sections
    
    def _split_text_by_batch_size(self, text: str) -> List[str]:
        """
        æŒ‰æ‰¹æ¬¡å¤§å°ç›´æ¥åˆ†å‰²æ–‡æœ¬
        
        Args:
            text: åŸå§‹æ–‡æ¡£æ–‡æœ¬
            
        Returns:
            åˆ†å‰²åçš„æ‰¹æ¬¡åˆ—è¡¨
        """
        # è®¡ç®—æ‰¹æ¬¡å¤§å°ï¼ˆåŸºäºæ¨¡å‹çš„tokené™åˆ¶ï¼‰
        batch_chars = self.max_chunk_chars  # å·²ç»è€ƒè™‘äº†reserved_tokens
        
        self.logger.info(f"ğŸ“ åˆ†å‰²å‚æ•°ï¼šæ–‡æ¡£{len(text)}å­—ç¬¦ï¼Œæ‰¹æ¬¡å¤§å°={batch_chars}å­—ç¬¦")
        
        # å¦‚æœæ–‡æ¡£å°äºæ‰¹æ¬¡å¤§å°ï¼Œç›´æ¥è¿”å›
        if len(text) <= batch_chars:
            self.logger.info(f"ğŸ“„ æ–‡æ¡£è¾ƒå°ï¼Œå•æ‰¹æ¬¡å¤„ç†")
            return [text]
        
        batches = []
        start = 0
        
        while start < len(text):
            # ç›´æ¥æŒ‰å›ºå®šé•¿åº¦åˆ†å‰²
            end = start + batch_chars
            batch_content = text[start:end]
            
            if batch_content:  # è·³è¿‡ç©ºå†…å®¹
                batches.append(batch_content)
                self.logger.debug(f"ğŸ“¦ æ‰¹æ¬¡ {len(batches)}: {len(batch_content)}å­—ç¬¦")
            
            start = end
        
        self.logger.info(f"âœ… æ–‡æ¡£åˆ†å‰²å®Œæˆï¼š{len(batches)}ä¸ªæ‰¹æ¬¡ï¼Œå¹³å‡{sum(len(b) for b in batches) // len(batches) if batches else 0}å­—ç¬¦/æ‰¹æ¬¡")
        return batches
    
    async def _extract_sections_from_batch(self, batch_text: str, batch_index: int) -> List[Dict]:
        """
        ä»å•ä¸ªæ‰¹æ¬¡ä¸­æå–ç« èŠ‚ï¼Œå¹¶æ ‡è®°å®Œæ•´æ€§
        
        Args:
            batch_text: æ‰¹æ¬¡æ–‡æœ¬å†…å®¹
            batch_index: æ‰¹æ¬¡ç´¢å¼•
            
        Returns:
            ç« èŠ‚åˆ—è¡¨ï¼ŒåŒ…å«å®Œæ•´æ€§æ ‡è®°
        """
        self.logger.info(f"ğŸ“„ å¤„ç†æ‰¹æ¬¡ {batch_index + 1}ï¼š{len(batch_text)}å­—ç¬¦")
        
        try:
            # åŠ è½½ç³»ç»Ÿæç¤º
            system_prompt = prompt_loader.get_system_prompt('document_preprocess')
            
            # æ„å»ºç”¨æˆ·æç¤º
            user_prompt = prompt_loader.get_user_prompt(
                'document_preprocess',
                format_instructions=self.structure_parser.get_format_instructions(),
                document_content=batch_text
            )
            
            # åˆ›å»ºæ¶ˆæ¯
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_prompt)
            ]
            
            # è°ƒç”¨AIæ¨¡å‹
            response = await self._call_ai_model(messages)
            
            # ä½¿ç”¨é‡è¯•è§£æå™¨è¿›è¡ŒJSONè§£æ
            async def ai_call_func():
                return response  # ç›´æ¥è¿”å›å·²æœ‰çš„å“åº”
            
            # å®šä¹‰JSONæå–å‡½æ•°
            def json_extractor(content: str) -> Dict[str, Any]:
                # æŸ¥æ‰¾JSONå†…å®¹
                json_match = re.search(r'\{.*\}', content, re.DOTALL)
                if json_match:
                    json_str = json_match.group()
                    result = json.loads(json_str)
                    # éªŒè¯ç»“æœç»“æ„
                    if 'sections' not in result:
                        result = {'sections': []}
                    return result
                else:
                    raise ValueError("æœªæ‰¾åˆ°JSONæ ¼å¼å†…å®¹")
            
            try:
                # ä½¿ç”¨å¸¦é‡è¯•çš„è§£æå™¨
                result = await ai_retry_parser.parse_json_with_retry(
                    ai_call_func=ai_call_func,
                    json_extractor=json_extractor,
                    retry_config=self.retry_config,
                    operation_name=f"è§£ææ‰¹æ¬¡ {batch_index + 1}"
                )
                
                sections = result.get('sections', [])
                
                if not sections:
                    # å¦‚æœè§£ææˆåŠŸä½†æ²¡æœ‰ç« èŠ‚ï¼Œåˆ›å»ºé»˜è®¤ç« èŠ‚
                    sections = [{
                        "section_title": f"æ‰¹æ¬¡ {batch_index + 1}",
                        "content": batch_text,
                        "level": 1,
                        "completeness_status": "incomplete"
                    }]
                    
            except Exception as e:
                self.logger.error(f"âŒ æ‰¹æ¬¡ {batch_index + 1} JSONè§£æå¤±è´¥ (åŒ…å«é‡è¯•): {str(e)}")
                # å¦‚æœé‡è¯•è§£æéƒ½å¤±è´¥ï¼Œä½¿ç”¨æ–‡æœ¬å›é€€æ–¹æ¡ˆ
                sections = self._parse_text_fallback(response.content)
                if not sections:
                    sections = [{
                        "section_title": f"æ‰¹æ¬¡ {batch_index + 1} (è§£æå¤±è´¥)",
                        "content": batch_text,
                        "level": 1,
                        "completeness_status": "incomplete"
                    }]
            
            self.logger.info(f"âœ… æ‰¹æ¬¡ {batch_index + 1} å¤„ç†å®Œæˆï¼šè¯†åˆ«åˆ° {len(sections)} ä¸ªç« èŠ‚")
            
            # è®°å½•å®Œæ•´æ€§ç»Ÿè®¡
            completeness_stats = {}
            for section in sections:
                status = section.get('completeness_status', 'unknown')
                completeness_stats[status] = completeness_stats.get(status, 0) + 1
            
            if completeness_stats:
                self.logger.info(f"ğŸ“Š æ‰¹æ¬¡ {batch_index + 1} å®Œæ•´æ€§ç»Ÿè®¡: {completeness_stats}")
            
            return sections
            
        except Exception as e:
            self.logger.error(f"âŒ æ‰¹æ¬¡ {batch_index + 1} å¤„ç†å¤±è´¥: {str(e)}")
            # è¿”å›é»˜è®¤ç« èŠ‚
            return [{
                "section_title": f"æ‰¹æ¬¡ {batch_index + 1} (å¤„ç†å¤±è´¥)",
                "content": batch_text,
                "level": 1,
                "completeness_status": "incomplete"
            }]
    
    
    def _parse_text_fallback(self, content: str) -> List[Dict]:
        """
        å½“JSONè§£æå¤±è´¥æ—¶çš„æ–‡æœ¬è§£æåå¤‡æ–¹æ¡ˆ
        
        Args:
            content: AIå“åº”å†…å®¹
            
        Returns:
            è§£æå‡ºçš„ç« èŠ‚åˆ—è¡¨
        """
        sections = []
        # ç®€å•çš„åŸºäºæ ‡é¢˜çš„æ–‡æœ¬åˆ†å‰²
        parts = re.split(r'\n(?=#{1,6}\s)', content)
        
        for i, part in enumerate(parts):
            if part.strip():
                title_match = re.match(r'^(#{1,6})\s*(.+)', part.strip())
                if title_match:
                    level = len(title_match.group(1))
                    title = title_match.group(2)
                    content_text = part[len(title_match.group(0)):].strip()
                else:
                    level = 1
                    title = f"ç« èŠ‚ {i+1}"
                    content_text = part.strip()
                
                if len(content_text) > 20:  # è¿‡æ»¤å¤ªçŸ­çš„å†…å®¹
                    sections.append({
                        "section_title": title,
                        "content": content_text,
                        "level": level
                    })
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆç« èŠ‚ï¼Œè¿”å›é»˜è®¤ç« èŠ‚
        if not sections:
            sections.append({
                "section_title": "æ–‡æ¡£å†…å®¹",
                "content": content,
                "level": 1
            })
        
        return sections
    
    async def _call_ai_model(self, messages):
        """
        è°ƒç”¨AIæ¨¡å‹ï¼ˆä»…åœ¨æ­¤æ–¹æ³•å†…è¿›è¡Œmockåˆ¤æ–­ï¼‰
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            
        Returns:
            AIæ¨¡å‹å“åº”
        """
        # ç›´æ¥è¿›è¡ŒçœŸå®çš„AIè°ƒç”¨
        return await asyncio.to_thread(self.model.invoke, messages)
    
