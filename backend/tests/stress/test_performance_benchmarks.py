"""
æ€§èƒ½åŸºå‡†æµ‹è¯•
æµ‹è¯•ç³»ç»Ÿåœ¨å„ç§è´Ÿè½½æ¡ä»¶ä¸‹çš„æ€§èƒ½è¡¨ç°
"""
import asyncio
import json
import pytest
import time
import threading
import queue
from typing import List, Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from datetime import datetime, timedelta
import statistics

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""
    total_requests: int
    successful_requests: int
    failed_requests: int
    total_time: float
    avg_response_time: float
    min_response_time: float
    max_response_time: float
    p95_response_time: float
    p99_response_time: float
    requests_per_second: float
    success_rate: float
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "total_requests": self.total_requests,
            "successful_requests": self.successful_requests,
            "failed_requests": self.failed_requests,
            "total_time": self.total_time,
            "avg_response_time": self.avg_response_time,
            "min_response_time": self.min_response_time,
            "max_response_time": self.max_response_time,
            "p95_response_time": self.p95_response_time,
            "p99_response_time": self.p99_response_time,
            "requests_per_second": self.requests_per_second,
            "success_rate": self.success_rate
        }


class LoadTestRunner:
    """è´Ÿè½½æµ‹è¯•æ‰§è¡Œå™¨"""
    
    def __init__(self, client, users: List[Dict]):
        self.client = client
        self.users = users
        self.results_queue = queue.Queue()
    
    def execute_load_test(
        self,
        test_function,
        concurrent_users: int,
        total_requests: int,
        test_duration_seconds: Optional[int] = None
    ) -> PerformanceMetrics:
        """
        æ‰§è¡Œè´Ÿè½½æµ‹è¯•
        
        Args:
            test_function: æµ‹è¯•å‡½æ•°
            concurrent_users: å¹¶å‘ç”¨æˆ·æ•°
            total_requests: æ€»è¯·æ±‚æ•°ï¼ˆå¦‚æœæŒ‡å®šäº†durationåˆ™å¿½ç•¥ï¼‰
            test_duration_seconds: æµ‹è¯•æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰
        """
        print(f"ğŸš€ å¼€å§‹è´Ÿè½½æµ‹è¯•:")
        print(f"   å¹¶å‘ç”¨æˆ·æ•°: {concurrent_users}")
        print(f"   æ€»è¯·æ±‚æ•°: {total_requests}")
        if test_duration_seconds:
            print(f"   æµ‹è¯•æŒç»­æ—¶é—´: {test_duration_seconds}ç§’")
        
        start_time = time.time()
        stop_event = threading.Event()
        
        # å¦‚æœæŒ‡å®šäº†æŒç»­æ—¶é—´ï¼Œè®¾ç½®å®šæ—¶å™¨
        if test_duration_seconds:
            timer = threading.Timer(test_duration_seconds, stop_event.set)
            timer.start()
        
        with ThreadPoolExecutor(max_workers=concurrent_users) as executor:
            futures = []
            requests_submitted = 0
            
            # æäº¤è¯·æ±‚
            while (not stop_event.is_set() and 
                   (test_duration_seconds or requests_submitted < total_requests)):
                
                if requests_submitted >= len(self.users) * 10:  # é¿å…æäº¤è¿‡å¤šè¯·æ±‚
                    break
                    
                user = self.users[requests_submitted % len(self.users)]
                future = executor.submit(self._execute_single_request, test_function, user, requests_submitted)
                futures.append(future)
                requests_submitted += 1
                
                # å¦‚æœåŸºäºæ—¶é—´çš„æµ‹è¯•ï¼Œç¨å¾®å»¶è¿Ÿé¿å…ç¬é—´æäº¤å¤ªå¤š
                if test_duration_seconds and requests_submitted % 10 == 0:
                    time.sleep(0.1)
            
            # åœæ­¢å®šæ—¶å™¨ï¼ˆå¦‚æœè¿˜åœ¨è¿è¡Œï¼‰
            if test_duration_seconds:
                timer.cancel()
                stop_event.set()
            
            # æ”¶é›†æ‰€æœ‰ç»“æœ
            results = []
            for future in as_completed(futures):
                try:
                    result = future.result(timeout=30)  # 30ç§’è¶…æ—¶
                    results.append(result)
                except Exception as e:
                    # è¶…æ—¶æˆ–å…¶ä»–å¼‚å¸¸çš„è¯·æ±‚è®°å½•ä¸ºå¤±è´¥
                    results.append({
                        "success": False,
                        "response_time": 30.0,
                        "error": str(e)
                    })
        
        total_time = time.time() - start_time
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        return self._calculate_metrics(results, total_time)
    
    def _execute_single_request(self, test_function, user: Dict, request_id: int) -> Dict:
        """æ‰§è¡Œå•ä¸ªè¯·æ±‚"""
        start_time = time.time()
        
        try:
            result = test_function(self.client, user, request_id)
            end_time = time.time()
            
            return {
                "success": result.get("success", False),
                "response_time": end_time - start_time,
                "status_code": result.get("status_code"),
                "request_id": request_id
            }
            
        except Exception as e:
            end_time = time.time()
            return {
                "success": False,
                "response_time": end_time - start_time,
                "error": str(e),
                "request_id": request_id
            }
    
    def _calculate_metrics(self, results: List[Dict], total_time: float) -> PerformanceMetrics:
        """è®¡ç®—æ€§èƒ½æŒ‡æ ‡"""
        if not results:
            raise ValueError("æ²¡æœ‰æµ‹è¯•ç»“æœæ•°æ®")
        
        successful_results = [r for r in results if r["success"]]
        failed_results = [r for r in results if not r["success"]]
        
        response_times = [r["response_time"] for r in results]
        response_times.sort()
        
        # è®¡ç®—ç™¾åˆ†ä½æ•°
        def percentile(data: List[float], p: float) -> float:
            if not data:
                return 0.0
            index = int((len(data) - 1) * p / 100)
            return data[index]
        
        return PerformanceMetrics(
            total_requests=len(results),
            successful_requests=len(successful_results),
            failed_requests=len(failed_results),
            total_time=total_time,
            avg_response_time=statistics.mean(response_times) if response_times else 0,
            min_response_time=min(response_times) if response_times else 0,
            max_response_time=max(response_times) if response_times else 0,
            p95_response_time=percentile(response_times, 95),
            p99_response_time=percentile(response_times, 99),
            requests_per_second=len(results) / total_time if total_time > 0 else 0,
            success_rate=len(successful_results) / len(results) if results else 0
        )


class TestPerformanceBenchmarks:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•ç±»"""
    
    @pytest.fixture(autouse=True)
    def setup_load_test_users(self, client, create_stress_test_users):
        """è®¾ç½®è´Ÿè½½æµ‹è¯•ç”¨æˆ·"""
        self.load_test_users = []
        
        # ä½¿ç”¨Mockç³»ç»Ÿåˆ›å»º50ä¸ªç”¨æˆ·ç”¨äºè´Ÿè½½æµ‹è¯•
        mock_users = create_stress_test_users(50)
        
        for i, user in enumerate(mock_users):
            auth_data = {"code": f"load_test_user_{i}_auth_code_{user.uid}"}
            response = client.post("/api/auth/thirdparty/login-legacy", json=auth_data)
            
            if response.status_code == 200:
                result = response.json()
                self.load_test_users.append({
                    "user_id": result["user"]["id"],
                    "token": result["access_token"],
                    "headers": {"Authorization": f"Bearer {result['access_token']}"}
                })
        
        self.load_runner = LoadTestRunner(client, self.load_test_users)
        
        assert len(self.load_test_users) >= 10, "éœ€è¦è‡³å°‘10ä¸ªç”¨æˆ·è¿›è¡Œè´Ÿè½½æµ‹è¯•"
    
    def create_benchmark_document(self, size_category: str = "small") -> tuple:
        """åˆ›å»ºä¸åŒå¤§å°çš„åŸºå‡†æµ‹è¯•æ–‡æ¡£"""
        
        base_content = "è¿™æ˜¯ä¸€ä¸ªç”¨äºæ€§èƒ½åŸºå‡†æµ‹è¯•çš„æ ‡å‡†æ–‡æ¡£å†…å®¹ã€‚å®ƒåŒ…å«äº†å®Œæ•´çš„ç»“æ„å’Œè¶³å¤Ÿçš„æ–‡å­—æ¥è¿›è¡Œæœ‰æ„ä¹‰çš„æµ‹è¯•ã€‚"
        
        size_configs = {
            "small": 1,      # ~1KB
            "medium": 10,    # ~10KB  
            "large": 50,     # ~50KB
            "xlarge": 100    # ~100KB
        }
        
        multiplier = size_configs.get(size_category, 1)
        content = base_content * multiplier * 20  # æ¯20æ¬¡é‡å¤çº¦1KB
        
        # æ·»åŠ ç»“æ„åŒ–å†…å®¹
        structured_content = f"""
# æ€§èƒ½åŸºå‡†æµ‹è¯•æ–‡æ¡£ ({size_category.upper()})

## æ–‡æ¡£æ¦‚è¿°
{content}

## ä¸»è¦å†…å®¹

### ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ä¿¡æ¯
{content}

### ç¬¬äºŒéƒ¨åˆ†ï¼šè¯¦ç»†åˆ†æ  
{content}

### ç¬¬ä¸‰éƒ¨åˆ†ï¼šæŠ€æœ¯ç»†èŠ‚
{content}

## ç»“è®ºå’Œå»ºè®®

### ä¸»è¦å‘ç°
{content}

### å®æ–½å»ºè®®
{content}

### åç»­æ­¥éª¤
{content}
        """
        
        return (f"benchmark_{size_category}.md", structured_content.encode('utf-8'), "text/markdown")
    
    @pytest.mark.stress
    def test_task_creation_benchmark(self, client):
        """ä»»åŠ¡åˆ›å»ºæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print(f"\nğŸ“ˆ ä»»åŠ¡åˆ›å»ºæ€§èƒ½åŸºå‡†æµ‹è¯•")
        
        def create_task_request(client, user: Dict, request_id: int) -> Dict:
            """å•ä¸ªä»»åŠ¡åˆ›å»ºè¯·æ±‚"""
            try:
                filename, content, content_type = self.create_benchmark_document("small")
                
                response = client.post(
                    "/api/tasks/",
                    files={"file": (filename, content, content_type)},
                    data={"description": f"åŸºå‡†æµ‹è¯•ä»»åŠ¡ {request_id}"},
                    headers=user["headers"]
                )
                
                return {
                    "success": response.status_code == 201,
                    "status_code": response.status_code,
                    "task_id": response.json().get("id") if response.status_code == 201 else None
                }
                
            except Exception as e:
                return {
                    "success": False,
                    "error": str(e)
                }
        
        # æ‰§è¡ŒåŸºå‡†æµ‹è¯•ï¼š10ä¸ªå¹¶å‘ç”¨æˆ·ï¼Œ100ä¸ªè¯·æ±‚
        metrics = self.load_runner.execute_load_test(
            test_function=create_task_request,
            concurrent_users=10,
            total_requests=100
        )
        
        # æ‰“å°åŸºå‡†ç»“æœ
        self._print_benchmark_results("ä»»åŠ¡åˆ›å»º", metrics)
        
        # åŸºå‡†æ–­è¨€
        assert metrics.success_rate >= 0.7, f"ä»»åŠ¡åˆ›å»ºæˆåŠŸç‡ä½äºåŸºå‡†: {metrics.success_rate*100:.1f}%"
        assert metrics.avg_response_time <= 2.0, f"ä»»åŠ¡åˆ›å»ºå¹³å‡å“åº”æ—¶é—´è¶…è¿‡åŸºå‡†: {metrics.avg_response_time:.2f}s"
        assert metrics.p95_response_time <= 5.0, f"ä»»åŠ¡åˆ›å»ºP95å“åº”æ—¶é—´è¶…è¿‡åŸºå‡†: {metrics.p95_response_time:.2f}s"
        assert metrics.requests_per_second >= 20, f"ä»»åŠ¡åˆ›å»ºRPSä½äºåŸºå‡†: {metrics.requests_per_second:.1f}/s"
    
    @pytest.mark.stress
    def test_task_query_benchmark(self, client):
        """ä»»åŠ¡æŸ¥è¯¢æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print(f"\nğŸ” ä»»åŠ¡æŸ¥è¯¢æ€§èƒ½åŸºå‡†æµ‹è¯•")
        
        # å…ˆåˆ›å»ºä¸€äº›ä»»åŠ¡ä¾›æŸ¥è¯¢
        for user in self.load_test_users[:5]:
            filename, content, content_type = self.create_benchmark_document("small")
            client.post(
                "/api/tasks/",
                files={"file": (filename, content, content_type)},
                data={"description": "æŸ¥è¯¢åŸºå‡†æµ‹è¯•ä»»åŠ¡"},
                headers=user["headers"]
            )
        
        def query_tasks_request(client, user: Dict, request_id: int) -> Dict:
            """å•ä¸ªä»»åŠ¡æŸ¥è¯¢è¯·æ±‚"""
            try:
                response = client.get(
                    "/api/tasks/",
                    headers=user["headers"]
                )
                
                return {
                    "success": response.status_code == 200,
                    "status_code": response.status_code,
                    "task_count": len(response.json()) if response.status_code == 200 else 0
                }
                
            except Exception as e:
                return {
                    "success": False,
                    "error": str(e)
                }
        
        # æ‰§è¡ŒåŸºå‡†æµ‹è¯•ï¼š15ä¸ªå¹¶å‘ç”¨æˆ·ï¼Œ200ä¸ªè¯·æ±‚
        metrics = self.load_runner.execute_load_test(
            test_function=query_tasks_request,
            concurrent_users=15,
            total_requests=200
        )
        
        # æ‰“å°åŸºå‡†ç»“æœ
        self._print_benchmark_results("ä»»åŠ¡æŸ¥è¯¢", metrics)
        
        # åŸºå‡†æ–­è¨€
        assert metrics.success_rate >= 0.8, f"ä»»åŠ¡æŸ¥è¯¢æˆåŠŸç‡ä½äºåŸºå‡†: {metrics.success_rate*100:.1f}%"
        assert metrics.avg_response_time <= 1.0, f"ä»»åŠ¡æŸ¥è¯¢å¹³å‡å“åº”æ—¶é—´è¶…è¿‡åŸºå‡†: {metrics.avg_response_time:.2f}s"
        assert metrics.p95_response_time <= 2.0, f"ä»»åŠ¡æŸ¥è¯¢P95å“åº”æ—¶é—´è¶…è¿‡åŸºå‡†: {metrics.p95_response_time:.2f}s"
        assert metrics.requests_per_second >= 50, f"ä»»åŠ¡æŸ¥è¯¢RPSä½äºåŸºå‡†: {metrics.requests_per_second:.1f}/s"
    
    @pytest.mark.stress
    def test_mixed_operations_benchmark(self, client):
        """æ··åˆæ“ä½œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print(f"\nğŸ”„ æ··åˆæ“ä½œæ€§èƒ½åŸºå‡†æµ‹è¯•")
        
        def mixed_operation_request(client, user: Dict, request_id: int) -> Dict:
            """æ··åˆæ“ä½œè¯·æ±‚"""
            try:
                # æ ¹æ®è¯·æ±‚IDé€‰æ‹©ä¸åŒçš„æ“ä½œç±»å‹
                operation_type = request_id % 4
                
                if operation_type == 0:
                    # 25% åˆ›å»ºä»»åŠ¡
                    filename, content, content_type = self.create_benchmark_document("small")
                    response = client.post(
                        "/api/tasks/",
                        files={"file": (filename, content, content_type)},
                        data={"description": f"æ··åˆåŸºå‡†æµ‹è¯•ä»»åŠ¡ {request_id}"},
                        headers=user["headers"]
                    )
                    return {
                        "success": response.status_code == 201,
                        "operation": "create",
                        "status_code": response.status_code
                    }
                
                elif operation_type == 1:
                    # 25% æŸ¥è¯¢ä»»åŠ¡åˆ—è¡¨
                    response = client.get("/api/tasks/", headers=user["headers"])
                    return {
                        "success": response.status_code == 200,
                        "operation": "list",
                        "status_code": response.status_code
                    }
                
                elif operation_type == 2:
                    # 25% æŸ¥è¯¢ç”¨æˆ·ä¿¡æ¯
                    response = client.get("/api/users/me", headers=user["headers"])
                    return {
                        "success": response.status_code == 200,
                        "operation": "user",
                        "status_code": response.status_code
                    }
                
                else:
                    # 25% æŸ¥è¯¢ç³»ç»ŸçŠ¶æ€
                    response = client.get("/api/system/health", headers=user["headers"])
                    return {
                        "success": response.status_code == 200,
                        "operation": "health",
                        "status_code": response.status_code
                    }
                    
            except Exception as e:
                return {
                    "success": False,
                    "operation": "error",
                    "error": str(e)
                }
        
        # æ‰§è¡ŒåŸºå‡†æµ‹è¯•ï¼š20ä¸ªå¹¶å‘ç”¨æˆ·ï¼ŒæŒç»­30ç§’
        metrics = self.load_runner.execute_load_test(
            test_function=mixed_operation_request,
            concurrent_users=20,
            total_requests=1000,  # è¿™ä¸ªåœ¨time-basedæµ‹è¯•ä¸­ä¼šè¢«å¿½ç•¥
            test_duration_seconds=30
        )
        
        # æ‰“å°åŸºå‡†ç»“æœ
        self._print_benchmark_results("æ··åˆæ“ä½œ", metrics)
        
        # åŸºå‡†æ–­è¨€
        assert metrics.success_rate >= 0.6, f"æ··åˆæ“ä½œæˆåŠŸç‡ä½äºåŸºå‡†: {metrics.success_rate*100:.1f}%"
        assert metrics.avg_response_time <= 3.0, f"æ··åˆæ“ä½œå¹³å‡å“åº”æ—¶é—´è¶…è¿‡åŸºå‡†: {metrics.avg_response_time:.2f}s"
        assert metrics.requests_per_second >= 15, f"æ··åˆæ“ä½œRPSä½äºåŸºå‡†: {metrics.requests_per_second:.1f}/s"
    
    @pytest.mark.stress 
    def test_large_file_processing_benchmark(self, client):
        """å¤§æ–‡ä»¶å¤„ç†æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print(f"\nğŸ“„ å¤§æ–‡ä»¶å¤„ç†æ€§èƒ½åŸºå‡†æµ‹è¯•")
        
        def large_file_task_request(client, user: Dict, request_id: int) -> Dict:
            """å¤§æ–‡ä»¶ä»»åŠ¡åˆ›å»ºè¯·æ±‚"""
            try:
                # æ ¹æ®è¯·æ±‚IDä½¿ç”¨ä¸åŒå¤§å°çš„æ–‡ä»¶
                size_types = ["medium", "large", "xlarge"]
                size_type = size_types[request_id % len(size_types)]
                
                filename, content, content_type = self.create_benchmark_document(size_type)
                
                response = client.post(
                    "/api/tasks/",
                    files={"file": (filename, content, content_type)},
                    data={"description": f"å¤§æ–‡ä»¶åŸºå‡†æµ‹è¯•({size_type}) {request_id}"},
                    headers=user["headers"]
                )
                
                return {
                    "success": response.status_code == 201,
                    "status_code": response.status_code,
                    "file_size": size_type,
                    "task_id": response.json().get("id") if response.status_code == 201 else None
                }
                
            except Exception as e:
                return {
                    "success": False,
                    "error": str(e)
                }
        
        # æ‰§è¡ŒåŸºå‡†æµ‹è¯•ï¼š5ä¸ªå¹¶å‘ç”¨æˆ·ï¼Œ30ä¸ªè¯·æ±‚ï¼ˆå¤§æ–‡ä»¶å¤„ç†è¾ƒæ…¢ï¼‰
        metrics = self.load_runner.execute_load_test(
            test_function=large_file_task_request,
            concurrent_users=5,
            total_requests=30
        )
        
        # æ‰“å°åŸºå‡†ç»“æœ
        self._print_benchmark_results("å¤§æ–‡ä»¶å¤„ç†", metrics)
        
        # å¤§æ–‡ä»¶å¤„ç†çš„åŸºå‡†è¦æ±‚ç›¸å¯¹å®½æ¾
        assert metrics.success_rate >= 0.80, f"å¤§æ–‡ä»¶å¤„ç†æˆåŠŸç‡ä½äºåŸºå‡†: {metrics.success_rate*100:.1f}%"
        assert metrics.avg_response_time <= 10.0, f"å¤§æ–‡ä»¶å¤„ç†å¹³å‡å“åº”æ—¶é—´è¶…è¿‡åŸºå‡†: {metrics.avg_response_time:.2f}s"
        assert metrics.p95_response_time <= 20.0, f"å¤§æ–‡ä»¶å¤„ç†P95å“åº”æ—¶é—´è¶…è¿‡åŸºå‡†: {metrics.p95_response_time:.2f}s"
    
    def _print_benchmark_results(self, test_name: str, metrics: PerformanceMetrics):
        """æ‰“å°åŸºå‡†æµ‹è¯•ç»“æœ"""
        print(f"\nğŸ“Š {test_name}æ€§èƒ½åŸºå‡†ç»“æœ:")
        print(f"   æ€»è¯·æ±‚æ•°: {metrics.total_requests}")
        print(f"   æˆåŠŸè¯·æ±‚: {metrics.successful_requests}")
        print(f"   å¤±è´¥è¯·æ±‚: {metrics.failed_requests}")
        print(f"   æˆåŠŸç‡: {metrics.success_rate*100:.1f}%")
        print(f"   æ€»è€—æ—¶: {metrics.total_time:.2f}s")
        print(f"   å¹³å‡å“åº”æ—¶é—´: {metrics.avg_response_time:.3f}s")
        print(f"   æœ€å°å“åº”æ—¶é—´: {metrics.min_response_time:.3f}s")
        print(f"   æœ€å¤§å“åº”æ—¶é—´: {metrics.max_response_time:.3f}s")
        print(f"   P95å“åº”æ—¶é—´: {metrics.p95_response_time:.3f}s")
        print(f"   P99å“åº”æ—¶é—´: {metrics.p99_response_time:.3f}s")
        print(f"   RPS (æ¯ç§’è¯·æ±‚æ•°): {metrics.requests_per_second:.1f}")
        print(f"   {'='*50}")


if __name__ == "__main__":
    pytest.main([__file__, "-v", "-m", "stress"])