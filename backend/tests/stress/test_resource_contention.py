"""
èµ„æºç«äº‰å’Œå¹¶å‘å®‰å…¨æµ‹è¯•
æµ‹è¯•ç³»ç»Ÿåœ¨é«˜å¹¶å‘æƒ…å†µä¸‹çš„èµ„æºç«äº‰å¤„ç†å’Œæ•°æ®ä¸€è‡´æ€§
"""
import asyncio
import json
import pytest
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any, Set
from collections import defaultdict, Counter
import random


class TestResourceContention:
    """èµ„æºç«äº‰æµ‹è¯•ç±»"""
    
    @pytest.fixture(autouse=True)
    def setup_resource_test_users(self, client, create_stress_test_users):
        """è®¾ç½®èµ„æºç«äº‰æµ‹è¯•ç”¨æˆ·"""
        self.resource_users = []
        
        # ä½¿ç”¨Mockç³»ç»Ÿåˆ›å»º50ä¸ªç”¨æˆ·ç”¨äºèµ„æºç«äº‰æµ‹è¯•
        mock_users = create_stress_test_users(50)
        
        for i, user in enumerate(mock_users):
            auth_data = {"code": f"resource_test_user_{i}_auth_code_{user.uid}"}
            response = client.post("/api/auth/thirdparty/login-legacy", json=auth_data)
            
            if response.status_code == 200:
                result = response.json()
                self.resource_users.append({
                    "user_id": result["user"]["id"],
                    "username": f"resource_user_{i}",
                    "token": result["access_token"],
                    "headers": {"Authorization": f"Bearer {result['access_token']}"}
                })
        
        assert len(self.resource_users) >= 10, "éœ€è¦è‡³å°‘10ä¸ªç”¨æˆ·è¿›è¡Œèµ„æºç«äº‰æµ‹è¯•"
    
    @pytest.mark.stress
    def test_concurrent_task_id_generation(self, client):
        """æµ‹è¯•å¹¶å‘ä»»åŠ¡IDç”Ÿæˆçš„å”¯ä¸€æ€§"""
        print(f"\nğŸ”¢ æµ‹è¯•å¹¶å‘ä»»åŠ¡IDç”Ÿæˆå”¯ä¸€æ€§")
        
        created_task_ids: Set[int] = set()
        id_creation_results = []
        lock = threading.Lock()
        
        def create_task_get_id(user_info: Dict, task_index: int) -> Dict:
            """åˆ›å»ºä»»åŠ¡å¹¶è·å–ID"""
            try:
                # åˆ›å»ºä»»åŠ¡
                filename = f"id_test_{task_index}.md"
                content = f"# ä»»åŠ¡IDå”¯ä¸€æ€§æµ‹è¯• {task_index}\n\nè¿™æ˜¯ç”¨äºæµ‹è¯•ä»»åŠ¡IDç”Ÿæˆå”¯ä¸€æ€§çš„æ–‡æ¡£å†…å®¹ã€‚ä»»åŠ¡åºå·ï¼š{task_index}"
                
                response = client.post(
                    "/api/tasks/",
                    files={"file": (filename, content.encode('utf-8'), "text/markdown")},
                    data={"description": f"IDå”¯ä¸€æ€§æµ‹è¯•ä»»åŠ¡ {task_index}"},
                    headers=user_info["headers"]
                )
                
                if response.status_code == 201:
                    task_data = response.json()
                    task_id = task_data["id"]
                    
                    # çº¿ç¨‹å®‰å…¨åœ°æ£€æŸ¥IDå”¯ä¸€æ€§
                    with lock:
                        is_duplicate = task_id in created_task_ids
                        if not is_duplicate:
                            created_task_ids.add(task_id)
                    
                    return {
                        "success": True,
                        "task_id": task_id,
                        "task_index": task_index,
                        "user_id": user_info["user_id"],
                        "is_duplicate": is_duplicate
                    }
                else:
                    return {
                        "success": False,
                        "task_index": task_index,
                        "user_id": user_info["user_id"],
                        "error": f"HTTP {response.status_code}: {response.text}"
                    }
                    
            except Exception as e:
                return {
                    "success": False,
                    "task_index": task_index,
                    "user_id": user_info["user_id"],
                    "error": str(e)
                }
        
        # å¹¶å‘åˆ›å»ºä»»åŠ¡
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=len(self.resource_users)) as executor:
            futures = []
            
            # æ¯ä¸ªç”¨æˆ·åˆ›å»º3ä¸ªä»»åŠ¡
            task_index = 0
            for user in self.resource_users:
                for i in range(3):
                    future = executor.submit(create_task_get_id, user, task_index)
                    futures.append(future)
                    task_index += 1
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(futures):
                result = future.result()
                id_creation_results.append(result)
        
        total_time = time.time() - start_time
        
        # åˆ†æIDå”¯ä¸€æ€§
        successful_creations = [r for r in id_creation_results if r["success"]]
        duplicate_ids = [r for r in successful_creations if r.get("is_duplicate", False)]
        unique_ids = len(created_task_ids)
        
        print(f"ğŸ“Š ä»»åŠ¡IDå”¯ä¸€æ€§æµ‹è¯•ç»“æœ:")
        print(f"   å°è¯•åˆ›å»ºä»»åŠ¡æ•°: {len(id_creation_results)}")
        print(f"   æˆåŠŸåˆ›å»ºä»»åŠ¡æ•°: {len(successful_creations)}")
        print(f"   å”¯ä¸€ä»»åŠ¡IDæ•°: {unique_ids}")
        print(f"   å‘ç°é‡å¤ID: {len(duplicate_ids)}")
        print(f"   æ€»è€—æ—¶: {total_time:.2f}ç§’")
        
        if duplicate_ids:
            print(f"âš ï¸ å‘ç°é‡å¤çš„ä»»åŠ¡ID:")
            for dup in duplicate_ids:
                print(f"     ä»»åŠ¡{dup['task_index']}: ID {dup['task_id']} (ç”¨æˆ·{dup['user_id']})")
        
        # æ–­è¨€ï¼šä¸åº”è¯¥æœ‰é‡å¤çš„ä»»åŠ¡ID
        assert len(duplicate_ids) == 0, f"å‘ç°äº†{len(duplicate_ids)}ä¸ªé‡å¤çš„ä»»åŠ¡ID"
        
        # æ–­è¨€ï¼šå”¯ä¸€IDæ•°é‡åº”è¯¥ç­‰äºæˆåŠŸåˆ›å»ºçš„ä»»åŠ¡æ•°
        assert unique_ids == len(successful_creations), \
            f"å”¯ä¸€IDæ•°é‡({unique_ids})ä¸ç­‰äºæˆåŠŸåˆ›å»ºçš„ä»»åŠ¡æ•°({len(successful_creations)})"
    
    @pytest.mark.stress
    def test_concurrent_user_session_management(self, client):
        """æµ‹è¯•å¹¶å‘ç”¨æˆ·ä¼šè¯ç®¡ç†"""
        print(f"\nğŸ‘¥ æµ‹è¯•å¹¶å‘ç”¨æˆ·ä¼šè¯ç®¡ç†")
        
        session_results = []
        
        def perform_user_operations(user_info: Dict, operation_count: int) -> Dict:
            """æ‰§è¡Œç”¨æˆ·æ“ä½œåºåˆ—"""
            operations_performed = []
            errors = []
            
            try:
                for i in range(operation_count):
                    # éšæœºé€‰æ‹©æ“ä½œç±»å‹
                    operation_type = random.choice(["profile", "tasks", "create_task"])
                    
                    if operation_type == "profile":
                        # è·å–ç”¨æˆ·èµ„æ–™
                        response = client.get("/api/users/me", headers=user_info["headers"])
                        operations_performed.append({
                            "operation": "profile",
                            "success": response.status_code == 200,
                            "response_time": 0  # ç®€åŒ–å¤„ç†
                        })
                        
                    elif operation_type == "tasks":
                        # è·å–ä»»åŠ¡åˆ—è¡¨
                        response = client.get("/api/tasks/", headers=user_info["headers"])
                        operations_performed.append({
                            "operation": "tasks",
                            "success": response.status_code == 200,
                            "task_count": len(response.json()) if response.status_code == 200 else 0
                        })
                        
                    elif operation_type == "create_task":
                        # åˆ›å»ºä»»åŠ¡
                        content = f"ä¼šè¯æµ‹è¯•ä»»åŠ¡ - ç”¨æˆ·{user_info['user_id']} - æ“ä½œ{i}"
                        response = client.post(
                            "/api/tasks/",
                            files={"file": (f"session_test_{i}.md", content.encode('utf-8'), "text/markdown")},
                            data={"description": content},
                            headers=user_info["headers"]
                        )
                        operations_performed.append({
                            "operation": "create_task",
                            "success": response.status_code == 201,
                            "task_id": response.json().get("id") if response.status_code == 201 else None
                        })
                    
                    # éšæœºå»¶è¿Ÿæ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¡Œä¸º
                    time.sleep(random.uniform(0.1, 0.3))
                
                successful_operations = [op for op in operations_performed if op.get("success", False)]
                
                return {
                    "user_id": user_info["user_id"],
                    "username": user_info["username"],
                    "total_operations": len(operations_performed),
                    "successful_operations": len(successful_operations),
                    "operations": operations_performed,
                    "success_rate": len(successful_operations) / len(operations_performed) if operations_performed else 0
                }
                
            except Exception as e:
                return {
                    "user_id": user_info["user_id"],
                    "username": user_info["username"],
                    "total_operations": len(operations_performed),
                    "successful_operations": 0,
                    "error": str(e),
                    "success_rate": 0
                }
        
        # å¹¶å‘æ‰§è¡Œç”¨æˆ·ä¼šè¯æ“ä½œ
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=len(self.resource_users)) as executor:
            futures = []
            
            for user in self.resource_users:
                # æ¯ä¸ªç”¨æˆ·æ‰§è¡Œ5-10ä¸ªéšæœºæ“ä½œ
                operation_count = random.randint(5, 10)
                future = executor.submit(perform_user_operations, user, operation_count)
                futures.append(future)
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(futures):
                result = future.result()
                session_results.append(result)
        
        total_time = time.time() - start_time
        
        # åˆ†æä¼šè¯ç®¡ç†ç»“æœ
        total_operations = sum(r["total_operations"] for r in session_results)
        total_successful = sum(r["successful_operations"] for r in session_results)
        
        # æŒ‰æ“ä½œç±»å‹ç»Ÿè®¡
        operation_stats = defaultdict(lambda: {"total": 0, "successful": 0})
        for result in session_results:
            for op in result.get("operations", []):
                op_type = op["operation"]
                operation_stats[op_type]["total"] += 1
                if op.get("success", False):
                    operation_stats[op_type]["successful"] += 1
        
        avg_success_rate = sum(r["success_rate"] for r in session_results) / len(session_results)
        
        print(f"ğŸ“Š å¹¶å‘ç”¨æˆ·ä¼šè¯ç®¡ç†æµ‹è¯•ç»“æœ:")
        print(f"   å¹¶å‘ç”¨æˆ·æ•°: {len(self.resource_users)}")
        print(f"   æ€»æ“ä½œæ•°: {total_operations}")
        print(f"   æˆåŠŸæ“ä½œæ•°: {total_successful}")
        print(f"   æ•´ä½“æˆåŠŸç‡: {total_successful/total_operations*100:.1f}%")
        print(f"   å¹³å‡ç”¨æˆ·æˆåŠŸç‡: {avg_success_rate*100:.1f}%")
        print(f"   æ€»è€—æ—¶: {total_time:.2f}ç§’")
        
        print(f"   æŒ‰æ“ä½œç±»å‹ç»Ÿè®¡:")
        for op_type, stats in operation_stats.items():
            success_rate = stats["successful"] / stats["total"] * 100 if stats["total"] > 0 else 0
            print(f"     {op_type}: {stats['successful']}/{stats['total']} ({success_rate:.1f}%)")
        
        # æ–­è¨€ï¼šæ•´ä½“æˆåŠŸç‡åº”è¯¥å¤§äº90%
        overall_success_rate = total_successful / total_operations if total_operations > 0 else 0
        assert overall_success_rate >= 0.90, \
            f"å¹¶å‘ç”¨æˆ·ä¼šè¯ç®¡ç†æ•´ä½“æˆåŠŸç‡è¿‡ä½: {overall_success_rate*100:.1f}%"
        
        # æ–­è¨€ï¼šæ¯ç§æ“ä½œçš„æˆåŠŸç‡éƒ½åº”è¯¥å¤§äº85%
        for op_type, stats in operation_stats.items():
            op_success_rate = stats["successful"] / stats["total"] if stats["total"] > 0 else 0
            assert op_success_rate >= 0.85, \
                f"{op_type}æ“ä½œæˆåŠŸç‡è¿‡ä½: {op_success_rate*100:.1f}%"
    
    @pytest.mark.stress
    def test_database_transaction_consistency(self, client):
        """æµ‹è¯•æ•°æ®åº“äº‹åŠ¡ä¸€è‡´æ€§"""
        print(f"\nğŸ—„ï¸ æµ‹è¯•æ•°æ®åº“äº‹åŠ¡ä¸€è‡´æ€§")
        
        # é¦–å…ˆä¸ºæ¯ä¸ªç”¨æˆ·åˆ›å»ºä¸€ä¸ªä»»åŠ¡ï¼Œç”¨äºåç»­å¹¶å‘æ“ä½œ
        base_task_ids = []
        for i, user in enumerate(self.resource_users[:5]):  # åªç”¨å‰5ä¸ªç”¨æˆ·
            content = f"äº‹åŠ¡ä¸€è‡´æ€§æµ‹è¯•åŸºç¡€ä»»åŠ¡ {i}"
            response = client.post(
                "/api/tasks/",
                files={"file": (f"transaction_test_{i}.md", content.encode('utf-8'), "text/markdown")},
                data={"description": content},
                headers=user["headers"]
            )
            
            if response.status_code == 201:
                base_task_ids.append(response.json()["id"])
        
        assert len(base_task_ids) >= 3, "éœ€è¦è‡³å°‘3ä¸ªåŸºç¡€ä»»åŠ¡è¿›è¡Œäº‹åŠ¡ä¸€è‡´æ€§æµ‹è¯•"
        
        transaction_results = []
        
        def perform_concurrent_task_operations(user_info: Dict, task_ids: List[int]) -> Dict:
            """æ‰§è¡Œå¹¶å‘ä»»åŠ¡æ“ä½œ"""
            operations = []
            
            try:
                for task_id in task_ids:
                    # å¯¹åŒä¸€ä¸ªä»»åŠ¡æ‰§è¡Œå¤šç§æ“ä½œ
                    
                    # 1. æŸ¥è¯¢ä»»åŠ¡è¯¦æƒ…
                    response1 = client.get(f"/api/tasks/{task_id}", headers=user_info["headers"])
                    operations.append({
                        "operation": "get_task",
                        "task_id": task_id,
                        "success": response1.status_code == 200,
                        "data": response1.json() if response1.status_code == 200 else None
                    })
                    
                    # 2. æŸ¥è¯¢ä»»åŠ¡çš„AIè¾“å‡º
                    response2 = client.get(f"/api/ai-outputs/task/{task_id}", headers=user_info["headers"])
                    operations.append({
                        "operation": "get_ai_outputs",
                        "task_id": task_id,
                        "success": response2.status_code == 200,
                        "output_count": len(response2.json()) if response2.status_code == 200 else 0
                    })
                    
                    # çŸ­æš‚å»¶è¿Ÿ
                    time.sleep(0.05)
                
                successful_ops = [op for op in operations if op["success"]]
                
                return {
                    "user_id": user_info["user_id"],
                    "operations": operations,
                    "successful_operations": len(successful_ops),
                    "total_operations": len(operations),
                    "consistency_issues": []  # ç¨ååˆ†æ
                }
                
            except Exception as e:
                return {
                    "user_id": user_info["user_id"],
                    "operations": operations,
                    "successful_operations": 0,
                    "total_operations": len(operations),
                    "error": str(e),
                    "consistency_issues": []
                }
        
        # å¹¶å‘æ‰§è¡Œäº‹åŠ¡æ“ä½œ
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=len(self.resource_users[:10])) as executor:
            futures = []
            
            # å¤šä¸ªç”¨æˆ·å¹¶å‘æ“ä½œç›¸åŒçš„ä»»åŠ¡é›†åˆ
            for user in self.resource_users[:10]:
                future = executor.submit(perform_concurrent_task_operations, user, base_task_ids)
                futures.append(future)
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(futures):
                result = future.result()
                transaction_results.append(result)
        
        total_time = time.time() - start_time
        
        # åˆ†æäº‹åŠ¡ä¸€è‡´æ€§
        total_operations = sum(r["total_operations"] for r in transaction_results)
        total_successful = sum(r["successful_operations"] for r in transaction_results)
        
        # æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§
        task_data_snapshots = defaultdict(list)
        for result in transaction_results:
            for op in result["operations"]:
                if op["operation"] == "get_task" and op["success"] and op["data"]:
                    task_id = op["task_id"]
                    # æ”¶é›†å…³é”®å­—æ®µçš„å€¼
                    snapshot = {
                        "id": op["data"].get("id"),
                        "status": op["data"].get("status"),
                        "created_at": op["data"].get("created_at"),
                        "user_id": op["data"].get("user_id")
                    }
                    task_data_snapshots[task_id].append(snapshot)
        
        # æ£€æŸ¥æ¯ä¸ªä»»åŠ¡çš„æ•°æ®æ˜¯å¦ä¸€è‡´
        consistency_issues = []
        for task_id, snapshots in task_data_snapshots.items():
            if len(snapshots) > 1:
                # æ£€æŸ¥å…³é”®å­—æ®µæ˜¯å¦åœ¨æ‰€æœ‰å¿«ç…§ä¸­éƒ½ä¸€è‡´
                first_snapshot = snapshots[0]
                for snapshot in snapshots[1:]:
                    for field in ["id", "status", "created_at", "user_id"]:
                        if first_snapshot[field] != snapshot[field]:
                            consistency_issues.append({
                                "task_id": task_id,
                                "field": field,
                                "values": [s[field] for s in snapshots]
                            })
                            break
        
        print(f"ğŸ“Š æ•°æ®åº“äº‹åŠ¡ä¸€è‡´æ€§æµ‹è¯•ç»“æœ:")
        print(f"   å¹¶å‘ç”¨æˆ·æ•°: {len(self.resource_users[:10])}")
        print(f"   æµ‹è¯•ä»»åŠ¡æ•°: {len(base_task_ids)}")
        print(f"   æ€»æ“ä½œæ•°: {total_operations}")
        print(f"   æˆåŠŸæ“ä½œæ•°: {total_successful}")
        print(f"   æ“ä½œæˆåŠŸç‡: {total_successful/total_operations*100:.1f}%")
        print(f"   æ•°æ®ä¸€è‡´æ€§é—®é¢˜: {len(consistency_issues)}")
        print(f"   æ€»è€—æ—¶: {total_time:.2f}ç§’")
        
        if consistency_issues:
            print(f"âš ï¸ å‘ç°çš„ä¸€è‡´æ€§é—®é¢˜:")
            for issue in consistency_issues[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"     ä»»åŠ¡{issue['task_id']} å­—æ®µ'{issue['field']}' å€¼ä¸ä¸€è‡´: {issue['values']}")
        
        # æ–­è¨€ï¼šä¸åº”è¯¥æœ‰æ•°æ®ä¸€è‡´æ€§é—®é¢˜
        assert len(consistency_issues) == 0, \
            f"å‘ç°äº†{len(consistency_issues)}ä¸ªæ•°æ®ä¸€è‡´æ€§é—®é¢˜"
        
        # æ–­è¨€ï¼šæ“ä½œæˆåŠŸç‡åº”è¯¥å¤§äº90%
        success_rate = total_successful / total_operations if total_operations > 0 else 0
        assert success_rate >= 0.90, \
            f"å¹¶å‘äº‹åŠ¡æ“ä½œæˆåŠŸç‡è¿‡ä½: {success_rate*100:.1f}%"
    
    @pytest.mark.stress
    def test_file_upload_resource_contention(self, client):
        """æµ‹è¯•æ–‡ä»¶ä¸Šä¼ èµ„æºç«äº‰"""
        print(f"\nğŸ“ æµ‹è¯•æ–‡ä»¶ä¸Šä¼ èµ„æºç«äº‰")
        
        upload_results = []
        uploaded_file_sizes = []
        
        def create_test_file_content(size_kb: int, file_id: int) -> bytes:
            """åˆ›å»ºæŒ‡å®šå¤§å°çš„æµ‹è¯•æ–‡ä»¶å†…å®¹"""
            base_content = f"# æ–‡ä»¶ä¸Šä¼ ç«äº‰æµ‹è¯• {file_id}\n\nè¿™æ˜¯ç”¨äºæµ‹è¯•æ–‡ä»¶ä¸Šä¼ èµ„æºç«äº‰çš„å†…å®¹ã€‚æ–‡ä»¶IDï¼š{file_id}\n\n"
            # æ¯100ä¸ªå­—ç¬¦å¤§çº¦1KB
            repeat_count = (size_kb * 1024) // len(base_content)
            return (base_content * repeat_count).encode('utf-8')
        
        def upload_file_concurrently(user_info: Dict, file_index: int) -> Dict:
            """å¹¶å‘ä¸Šä¼ æ–‡ä»¶"""
            start_time = time.time()
            
            try:
                # åˆ›å»ºä¸åŒå¤§å°çš„æ–‡ä»¶ï¼ˆ1-10KBï¼‰
                file_size_kb = random.randint(1, 10)
                file_content = create_test_file_content(file_size_kb, file_index)
                filename = f"concurrent_upload_{file_index}_{file_size_kb}kb.md"
                
                response = client.post(
                    "/api/tasks/",
                    files={"file": (filename, file_content, "text/markdown")},
                    data={"description": f"å¹¶å‘æ–‡ä»¶ä¸Šä¼ æµ‹è¯• {file_index} ({file_size_kb}KB)"},
                    headers=user_info["headers"]
                )
                
                end_time = time.time()
                
                return {
                    "success": response.status_code == 201,
                    "file_index": file_index,
                    "file_size_kb": file_size_kb,
                    "user_id": user_info["user_id"],
                    "upload_time": end_time - start_time,
                    "task_id": response.json().get("id") if response.status_code == 201 else None,
                    "status_code": response.status_code
                }
                
            except Exception as e:
                end_time = time.time()
                return {
                    "success": False,
                    "file_index": file_index,
                    "file_size_kb": 0,
                    "user_id": user_info["user_id"],
                    "upload_time": end_time - start_time,
                    "error": str(e)
                }
        
        # å¹¶å‘æ–‡ä»¶ä¸Šä¼ 
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=len(self.resource_users)) as executor:
            futures = []
            
            # æ¯ä¸ªç”¨æˆ·ä¸Šä¼ 2ä¸ªæ–‡ä»¶
            file_index = 0
            for user in self.resource_users:
                for i in range(2):
                    future = executor.submit(upload_file_concurrently, user, file_index)
                    futures.append(future)
                    file_index += 1
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(futures):
                result = future.result()
                upload_results.append(result)
                if result["success"]:
                    uploaded_file_sizes.append(result["file_size_kb"])
        
        total_time = time.time() - start_time
        
        # åˆ†ææ–‡ä»¶ä¸Šä¼ ç»“æœ
        successful_uploads = [r for r in upload_results if r["success"]]
        failed_uploads = [r for r in upload_results if not r["success"]]
        
        avg_upload_time = sum(r["upload_time"] for r in upload_results) / len(upload_results)
        max_upload_time = max(r["upload_time"] for r in upload_results)
        
        # æŒ‰æ–‡ä»¶å¤§å°åˆ†æ
        size_stats = Counter(uploaded_file_sizes)
        total_uploaded_size = sum(uploaded_file_sizes)
        
        print(f"ğŸ“Š æ–‡ä»¶ä¸Šä¼ èµ„æºç«äº‰æµ‹è¯•ç»“æœ:")
        print(f"   å¹¶å‘ä¸Šä¼ æ–‡ä»¶æ•°: {len(upload_results)}")
        print(f"   æˆåŠŸä¸Šä¼ : {len(successful_uploads)}")
        print(f"   å¤±è´¥ä¸Šä¼ : {len(failed_uploads)}")
        print(f"   ä¸Šä¼ æˆåŠŸç‡: {len(successful_uploads)/len(upload_results)*100:.1f}%")
        print(f"   æ€»ä¸Šä¼ å¤§å°: {total_uploaded_size}KB")
        print(f"   å¹³å‡ä¸Šä¼ æ—¶é—´: {avg_upload_time:.2f}ç§’")
        print(f"   æœ€å¤§ä¸Šä¼ æ—¶é—´: {max_upload_time:.2f}ç§’")
        print(f"   æ€»è€—æ—¶: {total_time:.2f}ç§’")
        
        print(f"   æ–‡ä»¶å¤§å°åˆ†å¸ƒ:")
        for size_kb, count in sorted(size_stats.items()):
            print(f"     {size_kb}KB: {count}ä¸ªæ–‡ä»¶")
        
        # æ–­è¨€ï¼šæ–‡ä»¶ä¸Šä¼ æˆåŠŸç‡åº”è¯¥å¤§äº85%
        upload_success_rate = len(successful_uploads) / len(upload_results) if upload_results else 0
        assert upload_success_rate >= 0.85, \
            f"å¹¶å‘æ–‡ä»¶ä¸Šä¼ æˆåŠŸç‡è¿‡ä½: {upload_success_rate*100:.1f}%"
        
        # æ–­è¨€ï¼šå¹³å‡ä¸Šä¼ æ—¶é—´ä¸åº”è¯¥å¤ªé•¿
        assert avg_upload_time <= 3.0, \
            f"å¹³å‡æ–‡ä»¶ä¸Šä¼ æ—¶é—´è¿‡é•¿: {avg_upload_time:.2f}ç§’"


if __name__ == "__main__":
    pytest.main([__file__, "-v", "-m", "stress"])